{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology for testing further removal methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpDictionaries import PosMapper, PosList, Contraction_Dictionary2, stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re, nltk\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries for cleaning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dictionary includes contractions and their associated expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contraction_Dictionary1 = {\n",
    "    \"ain/t\": \"is not\", \"aren/t\": \"are not\", \"can/t\": \"can not\", \"can/t/ve\": \"can not have\", \"cause\": \"because\", \"could/ve\": \"could have\",\n",
    "    \"couldn/t\": \"could not\", \"couldn/t/ve\": \"could not have\", \"didn/t\": \"did not\", \"doesn/t\": \"does not\", \"don/t\": \"do not\", \"hadn/t\": \"had not\",\n",
    "    \"hadn/t/ve\": \"had not have\", \"hasn/t\": \"has not\", \"haven/t\": \"have not\", \"he/d\": \"he would\", \"he/d/ve\": \"he would have\", \"he/ll\": \"he will\",\n",
    "    \"he/ll/ve\": \"he he will have\", \"he/s\": \"he is\", \"how/d\": \"how did\", \"how/d/y\": \"how do you\", \"how/ll\": \"how will\", \"how/s\": \"how is\",\n",
    "    \"I/d\": \"I would\", \"I/d/ve\": \"I would have\", \"I/ll\": \"I will\", \"I/ll/ve\": \"I will have\", \"I/m\": \"I am\", \"I/ve\": \"I have\", \"i/d\": \"i would\",\n",
    "    \"i/d/ve\": \"i would have\", \"i/ll\": \"i will\", \"i/ll/ve\": \"i will have\", \"i/m\": \"i am\", \"i/ve\": \"i have\", \"isn/t\": \"is not\", \"it/d\": \"it would\",\n",
    "    \"it/d/ve\": \"it would have\", \"it/ll\": \"it will\", \"it/ll/ve\": \"it will have\", \"it/s\": \"it is\", \"let/s\": \"let us\", \"ma/am\": \"madam\", \"mayn/t\": \"may not\",\n",
    "    \"might/ve\": \"might have\", \"mightn/t\": \"might not\", \"mightn/t/ve\": \"might not have\", \"must/ve\": \"must have\", \"mustn/t\": \"must not\", \"mustn/t/ve\": \"must not have\",\n",
    "    \"needn/t\": \"need not\", \"needn/t/ve\": \"need not have\", \"o/clock\": \"of the clock\", \"oughtn/t\": \"ought not\", \"oughtn/t/ve\": \"ought not have\", \"shan/t\": \"shall not\",\n",
    "    \"sha/n/t\": \"shall not\", \"shan/t/ve\": \"shall not have\", \"she/d\": \"she would\", \"she/d/ve\": \"she would have\", \"she/ll\": \"she will\", \"she/ll/ve\": \"she will have\",\n",
    "    \"she/s\": \"she is\", \"should/ve\": \"should have\", \"shouldn/t\": \"should not\", \"shouldn/t/ve\": \"should not have\", \"so/ve\": \"so have\", \"so/s\": \"so as\",\n",
    "    \"that/d\": \"that would\", \"that/d/ve\": \"that would have\", \"that/s\": \"that is\", \"there/d\": \"there would\", \"there/d/ve\": \"there would have\",\n",
    "    \"there/s\": \"there is\", \"they/d\": \"they would\", \"they/d/ve\": \"they would have\", \"they/ll\": \"they will\", \"they/ll/ve\": \"they will have\", \"they/re\": \"they are\",\n",
    "    \"they/ve\": \"they have\", \"to/ve\": \"to have\", \"wasn/t\": \"was not\", \"we/d\": \"we would\", \"we/d/ve\": \"we would have\", \"we/ll\": \"we will\", \"we/ll/ve\": \"we will have\", \n",
    "    \"we/re\": \"we are\", \"we/ve\": \"we have\", \"weren/t\": \"were not\", \"what/ll\": \"what will\", \"what/ll/ve\": \"what will have\",\"what/re\": \"what are\", \"what/s\": \"what is\", \n",
    "    \"what/ve\": \"what have\", \"when/s\": \"when is\", \"when/ve\": \"when have\", \"where/d\": \"where did\", \"where/s\": \"where is\", \"where/ve\": \"where have\",\n",
    "    \"who/ll\": \"who will\", \"who/ll/ve\": \"who will have\", \"who/s\": \"who is\", \"who/ve\": \"who have\", \"why/s\": \"why is\", \"why/ve\": \"why have\", \"will/ve\": \"will have\", \n",
    "    \"won/t\": \"will not\",\"won/t/ve\": \"will not have\", \"would/ve\": \"would have\", \"wouldn/t\": \"would not\", \"wouldn/t/ve\": \"would not have\", \"y/all\": \"you all\",\n",
    "    \"y/all/d\": \"you all would\", \"y/all/d/ve\": \"you all would have\", \"y/all/re\": \"you all are\", \"y/all/ve\": \"you all have\", \"you/d\": \"you would\",\n",
    "    \"you/d/ve\": \"you would have\", \"you/ll\": \"you will\", \"you/ll/ve\": \"you will have\", \"you/re\": \"you are\", \"you/ve\": \"you have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list contains an edited list of stopwords, with all negation words (e.g. 'no', 'never', 'not') excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself',\n",
    "            'yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself',\n",
    "            'they','them','their','theirs','themselves','what','which','who','whom','this','that',\n",
    "            'these','those','am','is','are','was','were','be','been','being','have','has','had',\n",
    "            'having','do','does','did','doing','a','an','the','and','but','if','or','because','as',\n",
    "            'until','while','of','at','by','for','with','about','against','between','into','through',\n",
    "            'during','before','after','above','below','to','from','up','down','in','out','on','off',\n",
    "            'over','under','again','further','then','once','here','there','when','where','why','how',\n",
    "            'all','any','both','each','few','more','most','other','some','such',\n",
    "            'only','own','same','so','than','too','very','can','will','just','should',\n",
    "            'now','uses','use','using','used','one','also']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second list contains the nltk.wordnet labelling convertion for verbs, adjectives, nouns and adverbs. The purpose of this list is to only lemmatize words that are POS (part-of-speech) tagged with these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "PosList =[\"JJ\",\"JJR\",\"JJS\",\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"RB\",\n",
    "          \"RBR\",\"RBS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second dictionary uses the POS tag label as a key to refer to the root/lemma of a word. The purpose of this is to identify words with these POS tags and lemmatize them to their root lemma. E.g. 'running' --> 'run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "PosMapper = {\n",
    "\"JJ\": wordnet.ADJ,\n",
    "\"JJR\": wordnet.ADJ,\n",
    "\"JJS\": wordnet.ADJ,\n",
    "\"NN\": wordnet.NOUN,\n",
    "\"NNS\": wordnet.NOUN,\n",
    "\"NNP\": wordnet.NOUN,\n",
    "\"NNPS\": wordnet.NOUN,\n",
    "\"RB\": wordnet.ADV,\n",
    "\"RBR\": wordnet.ADV,\n",
    "\"RBS\": wordnet.ADV,\n",
    "\"VB\": wordnet.VERB,\n",
    "\"VBD\": wordnet.VERB,\n",
    "\"VBG\": wordnet.VERB,\n",
    "\"VBN\": wordnet.VERB,\n",
    "\"VBP\": wordnet.VERB,\n",
    "\"VBZ\": wordnet.VERB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Innitialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization/standardization method\n",
    "\n",
    "Method amended to selectively remove some punctuation. Other complicated puntuation is dealt with by separately defined methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method normalizes the text into a coherent format for matching\n",
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower() # Convert to lowercase\n",
    "    df[text_field] = df[text_field].str.replace('http','') # removing urls is useful to make vocabulary small as possible\n",
    "    df[text_field] = df[text_field].str.replace('com', '') # same as above.\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\") #  replacing at sign for a word\n",
    "    df[text_field] = df[text_field].str.replace(\".\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(\",\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(\"-\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(\"(\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(\")\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace('\"', \" \")\n",
    "    df[text_field] = df[text_field].str.replace(\"?\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(\"!\", \"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions Expansion Prep\n",
    "In the data, contraction words such as wouldn't are noted as 'wouldn`t' ` which is a different character to the normal apostrophe. Therefore each instance is changed to a '/' in order to match contractions to the contraction dictionary equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method strips the ` and changes is to / in order to match contractions.\n",
    "def contractionPrep(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lstrip(' ')\n",
    "    df[text_field] = df[text_field].str.replace(\"`\", '/')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contraction Expansion Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method expands all contractions to their original format\n",
    "def expand_contractions(text, contraction_mapping=Contraction_Dictionary2):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underscore Removal Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def underscoreCleaner(list_words):\n",
    "    for tweet in list_words:\n",
    "        for token in tweet:\n",
    "            token = token.replace('_',' ').strip() # replace underscore and strip leading and trailing spaces for each string\n",
    "    return list_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    listOfTokens = []\n",
    "    for text in x:\n",
    "        text = str(text)\n",
    "        text = word_tokenize(text)\n",
    "        listOfTokens.append(text)\n",
    "    return listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double letter removal Method\n",
    "After an innitial inspection into word frequency, single letter words were very frequent and didnt seem to contribute much semantic meaning the tweets, so were therefore removed.\n",
    "\n",
    "- Method has been amended to remove two-lettered words because they contribute no meaning to tweets. However it means we loose cases like 'xx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleLetterRemoval(list_object):\n",
    "    listOfTokens = []\n",
    "    for tweet in list_object:\n",
    "        temp = []\n",
    "        for word in tweet:\n",
    "            if len(word) > 2:\n",
    "                temp.append(word)\n",
    "        listOfTokens.append(temp)\n",
    "    return listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number Removal Method\n",
    "Similarly to single letters, numbers dont contribute much meaning to the polarity of a tweet and so therefore removed.\n",
    "\n",
    "- Method has been amended to remove every instance of numbers and all leading and trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberRemoval(list_object):\n",
    "    pattern = '[0-9]'\n",
    "    for tweet in list_object:\n",
    "        for token in tweet:\n",
    "            token = re.sub(pattern, '', token).strip()\n",
    "    return list_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix lengthening of words\n",
    "\n",
    "Method that fixes instances such as 'arhhh' --> 'ah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(list_object):\n",
    "    for tweet in list_object:\n",
    "        for token in tweet:\n",
    "            pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "            token = pattern.sub(r\"\\1\\1\", token)\n",
    "    return list_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Check Method\n",
    "\n",
    "Method only returns words listed in the English Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellChecker(list_object):\n",
    "    tokens = []\n",
    "    import enchant # This is a python spell checker form the PyEnchant library \n",
    "    d = enchant.Dict(\"en_Uk\")\n",
    "    for token in list_object:\n",
    "        if d.check(token) == True:\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal Method\n",
    "Stopwords are the most frequent words in the corpus and only create noise for the classifier so were therefore removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordRemoval(list_object):\n",
    "    listOfTokens = []\n",
    "    for tweet in list_object:\n",
    "        temp = []\n",
    "        for word in tweet:\n",
    "            if not word in stop_words:\n",
    "                temp.append(word)\n",
    "        listOfTokens.append(temp)\n",
    "    return listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Check Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spell checker test for lists of list of tokens\n",
    "def spellChecker(list_object):\n",
    "    tokens = []\n",
    "    import enchant # This is a python spell checker form the PyEnchant library \n",
    "    d = enchant.Dict(\"en_Uk\")\n",
    "    for tweet in list_object:\n",
    "        temp = []\n",
    "        for token in tweet:\n",
    "            if d.check(token) == True:\n",
    "                temp.append(token)\n",
    "        tokens.append(temp)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization Method\n",
    "Calls the Pos list and dictionary to return certain words into their root lemma format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(list_object):\n",
    "    tags = []\n",
    "    for words in list_object:\n",
    "        posTupples = nltk.pos_tag(words)\n",
    "        text = [lemmatizer.lemmatize(k[0], pos=PosMapper.get(k[1])) if k[1] in PosList else k[0] for k in posTupples]\n",
    "        tags.append(text)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataset and clean it.\n",
    "In order to compare the raw dataset to the cleaned version, two datasets are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data to clean\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "#Read in raw dataset to test later\n",
    "data2 = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'selected text' column\n",
    "data = data.drop(columns='selected_text')\n",
    "data2 = data2.drop(columns='selected_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize datasets\n",
    "Each dataset is standardized (puntuation removed, converted to lower case, url-like stuff removed etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>i`d have responded  if i were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>sons of       why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  cb774db0d1                i`d have responded  if i were going   neutral\n",
       "1  549e992a42         sooo sad i will miss you here in san diego  negative\n",
       "2  088c60f138                          my boss is bullying me     negative\n",
       "3  9642c003ef                      what interview leave me alone  negative\n",
       "4  358bd9e861   sons of       why couldn`t they put them on t...  negative"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize Text\n",
    "data = standardize_text(data,'text')\n",
    "data2 = standardize_text(data2,'text')\n",
    "data2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand contractions\n",
    "Only the dataset that is being cleaned calls these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data ready for Contraction Expansion\n",
    "data = contractionPrep(data,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>i/d have responded  if i were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>sons of       why couldn/t they put them on th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  cb774db0d1                i/d have responded  if i were going   neutral\n",
       "1  549e992a42         sooo sad i will miss you here in san diego  negative\n",
       "2  088c60f138                          my boss is bullying me     negative\n",
       "3  9642c003ef                      what interview leave me alone  negative\n",
       "4  358bd9e861  sons of       why couldn/t they put them on th...  negative"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand Contractions\n",
    "cleanedData = [expand_contractions(str(tweet)) for tweet in data['text']]\n",
    "data['text'] = cleanedData\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the results there were still some square brackets remaining as part of some words so these needed to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip remaining / from data\n",
    "data['text'] = data['text'].str.replace('/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id have responded  if i were going', 'sooo sad i will miss you here in san diego', 'my boss is bullying me   ', 'what interview leave me alone', 'sons of       why couldnt they put them on the releases we already bought']\n"
     ]
    }
   ],
   "source": [
    "token_list = data['text'].tolist()\n",
    "print(token_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id have responded  if i were going', 'sooo sad i will miss you here in san diego', 'my boss is bullying me   ', 'what interview leave me alone', 'sons of       why couldnt they put them on the releases we already bought']\n"
     ]
    }
   ],
   "source": [
    "#Convert list of tokens to string\n",
    "list_corpus = []\n",
    "for tokens in token_list:\n",
    "    tokens = tokens.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\",\",\"\")\n",
    "    list_corpus.append(tokens)\n",
    "print(list_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['id', 'have', 'responded', 'if', 'i', 'were', 'going'], ['sooo', 'sad', 'i', 'will', 'miss', 'you', 'here', 'in', 'san', 'diego'], ['my', 'boss', 'is', 'bullying', 'me'], ['what', 'interview', 'leave', 'me', 'alone'], ['sons', 'of', 'why', 'couldnt', 'they', 'put', 'them', 'on', 'the', 'releases', 'we', 'already', 'bought']]\n"
     ]
    }
   ],
   "source": [
    "listOfTokens = tokenizer(list_corpus) #change to data['text'] for original word count\n",
    "print(listOfTokens[:5]) #Print what text data looks like in first tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356359 tokens total, with a vocabulary size of 26590\n"
     ]
    }
   ],
   "source": [
    "#Create Vocabulary\n",
    "all_words = [token for tokens in listOfTokens for token in tokens]\n",
    "vocab1 = sorted(list(set(all_words)))\n",
    "print(\"%s tokens total, with a vocabulary size of %s\" % (len(all_words), len(vocab1)))\n",
    "#sentence_lengths = [len(tokens) for tokens in listOfTokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Data\n",
    "This is applied to both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Data\n",
    "data['text'] = listOfTokens\n",
    "\n",
    "#Tokenize raw dataset without cleaning methods\n",
    "data2['text'] = listOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>[id, have, responded, if, i, were, going]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  cb774db0d1          [id, have, responded, if, i, were, going]   neutral\n",
       "1  549e992a42  [sooo, sad, i, will, miss, you, here, in, san,...  negative"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>[id, have, responded, if, i, were, going]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  cb774db0d1          [id, have, responded, if, i, were, going]   neutral\n",
       "1  549e992a42  [sooo, sad, i, will, miss, you, here, in, san,...  negative"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call rest of cleaning methods on the dataset that is being cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       textID                                    text sentiment\n",
      "0  cb774db0d1                      [responded, going]   neutral\n",
      "1  549e992a42                             [sad, miss]  negative\n",
      "2  088c60f138                        [boss, bullying]  negative\n",
      "3  9642c003ef               [interview, leave, alone]  negative\n",
      "4  358bd9e861  [sons, put, releases, already, bought]  negative\n"
     ]
    }
   ],
   "source": [
    "#Underscore Removal:\n",
    "tweetData = data['text'].tolist()\n",
    "usRemoved = underscoreCleaner(tweetData)\n",
    "data['text'] = usRemoved\n",
    "\n",
    "#Double letter removal:\n",
    "tweetData = data['text'].tolist()\n",
    "slRemoved = doubleLetterRemoval(tweetData)\n",
    "data['text'] = slRemoved\n",
    "\n",
    "#number removal\n",
    "tweetData = data['text'].tolist()\n",
    "nRemoved = numberRemoval(tweetData)\n",
    "data['text'] = nRemoved\n",
    "\n",
    "#stopword removal\n",
    "tweetData = data['text'].tolist()\n",
    "noiseRemoved = stopwordRemoval(tweetData)\n",
    "data['text'] = noiseRemoved\n",
    "\n",
    "\n",
    "#spell checker\n",
    "tweetData = data['text'].tolist()\n",
    "checked = spellChecker(tweetData)\n",
    "data['text'] = checked\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize the cleaned Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Lemmatize Data\n",
    "tweetData = data['text'].tolist()\n",
    "lemmatizedData = lemma(tweetData)\n",
    "data['text'] = lemmatizedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>[respond, go]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>[sad, miss]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>[boss, bully]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>[son, put, release, already, buy]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                               text sentiment\n",
       "0  cb774db0d1                      [respond, go]   neutral\n",
       "1  549e992a42                        [sad, miss]  negative\n",
       "2  088c60f138                      [boss, bully]  negative\n",
       "3  9642c003ef          [interview, leave, alone]  negative\n",
       "4  358bd9e861  [son, put, release, already, buy]  negative"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5) # cleanDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>[id, have, responded, if, i, were, going]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>[my, boss, is, bullying, me]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>[what, interview, leave, me, alone]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>[sons, of, why, couldnt, they, put, them, on, ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  cb774db0d1          [id, have, responded, if, i, were, going]   neutral\n",
       "1  549e992a42  [sooo, sad, i, will, miss, you, here, in, san,...  negative\n",
       "2  088c60f138                       [my, boss, is, bullying, me]  negative\n",
       "3  9642c003ef                [what, interview, leave, me, alone]  negative\n",
       "4  358bd9e861  [sons, of, why, couldnt, they, put, them, on, ...  negative"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head(5) # rawDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataframes to csv files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to CSV\n",
    "#data.to_csv(r'C:\\Users\\Alex\\Desktop\\fdmNLPproject\\cleanedData2.csv', index=False)\n",
    "#data2.to_csv(r'C:\\Users\\Alex\\Desktop\\fdmNLPproject\\rawData3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id have responded  if i were going', 'sooo sad i will miss you here in san diego', 'my boss is bullying me   ', 'what interview leave me alone', 'sons of       why couldnt they put them on the releases we already bought']\n"
     ]
    }
   ],
   "source": [
    "#Inspect vocabularies of newly cleaned dataset:\n",
    "listOfTokens = data['text'].tolist()\n",
    "print(token_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After checking whether words are in UK dictionary: 155737 tokens total, with a vocabulary size of 9651\n"
     ]
    }
   ],
   "source": [
    "#Create Vocabulary\n",
    "all_words = [token for tokens in listOfTokens for token in tokens]\n",
    "vocab1 = sorted(list(set(all_words)))\n",
    "print(\"After checking whether words are in UK dictionary: %s tokens total, with a vocabulary size of %s\" % (len(all_words), len(vocab1)))\n",
    "#sentence_lengths = [len(tokens) for tokens in listOfTokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of corpus before further word removal: 198338 tokens total, with a vocabulary size of 23262\n",
    "Vocab size down by 252 token types\n",
    "\n",
    "Original size corpus: 356359 tokens total, with a vocabulary size of 26590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
