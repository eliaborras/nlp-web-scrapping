{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Logistic Regression Models</center></h1>\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Bag-of-Words Vectoriser\n",
    "## Standard Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATA - BoW:\n",
      "\n",
      "Logistic Regression Accuracy:\n",
      " 0.702037351443124 \n",
      "\n",
      "Logistic Regression Precision:\n",
      " 0.7120952274036435 \n",
      "\n",
      "Logistic Regression Recall:\n",
      " 0.6981493574421952 \n",
      "\n",
      "Logistic Regression F1 Score:\n",
      " 0.703640156051177\n"
     ]
    }
   ],
   "source": [
    "                        # Importing needed packages\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import requests\n",
    "import io\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "# Training csv file\n",
    "url_train = \"https://raw.githubusercontent.com/SoniaLei/nlp-web-scrapping/development/data/raw/tweets-train.csv\"\n",
    "csv_train = requests.get(url_train).content\n",
    "df_train = pd.read_csv(io.StringIO(csv_train.decode('utf-8')))\n",
    "\n",
    "# Testing csv file\n",
    "url_test = \"https://raw.githubusercontent.com/SoniaLei/nlp-web-scrapping/development/data/raw/tweets-test.csv\"\n",
    "csv_test = requests.get(url_test).content\n",
    "df_test = pd.read_csv(io.StringIO(csv_test.decode('utf-8')))\n",
    "\n",
    "X_train = df_train['text'].astype(str)\n",
    "Y_train = df_train['sentiment'].astype(str)\n",
    "\n",
    "X_test = df_test['text'].astype(str)\n",
    "Y_test = df_test['sentiment'].astype(str)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom cleaner function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Bag-of-Words Vectoriser\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = text_cleaner, ngram_range=(1,1))\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Building OVR Logisitic Regression Classifier\n",
    "\n",
    "LogReg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "ovr = OneVsRestClassifier(LogReg)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', ovr)])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"RAW DATA - BoW:\\n\")\n",
    "print(\"Logistic Regression Accuracy:\\n\",metrics.accuracy_score(Y_test, predicted),\"\\n\") # Accuracy\n",
    "print(\"Logistic Regression Precision:\\n\",metrics.precision_score(Y_test, predicted, average='macro'),\"\\n\") # Precision\n",
    "print(\"Logistic Regression Recall:\\n\",metrics.recall_score(Y_test, predicted, average='macro'),\"\\n\") # Recall\n",
    "print(\"Logistic Regression F1 Score:\\n\",metrics.f1_score(Y_test, predicted, average='macro')) # F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Bag-of-Words Vectoriser\n",
    "## K-Fold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATA - BoW:\n",
      "\n",
      "[0.68601055 0.69104803 0.69905386 0.68995633 0.68795488]\n"
     ]
    }
   ],
   "source": [
    "                        # Importing needed packages\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/SoniaLei/nlp-web-scrapping/development/data/raw/tweets-train.csv\"\n",
    "csv = requests.get(url).content\n",
    "df = pd.read_csv(io.StringIO(csv.decode('utf-8')))\n",
    "\n",
    "X = df['text'].astype(str)\n",
    "Y = df['sentiment'].astype(str)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom cleaner function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Bag-of-Words Vectoriser\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = text_cleaner, ngram_range=(1,1))\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Building OVR Logisitic Regression Classifier\n",
    "\n",
    "LogReg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "ovr = OneVsRestClassifier(LogReg)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', ovr)])\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "print(\"RAW DATA - BoW:\\n\")\n",
    "print(cross_val_score(estimator=pipe, X=X, y=Y, cv=k_fold, scoring='accuracy', n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# TF-IDF Vectoriser\n",
    "## Standard Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATA - TF-IDF:\n",
      "\n",
      "Logistic Regression Accuracy:\n",
      " 0.7034521788341822 \n",
      "\n",
      "Logistic Regression Precision:\n",
      " 0.7237150860216944 \n",
      "\n",
      "Logistic Regression Recall:\n",
      " 0.6952970269380061 \n",
      "\n",
      "Logistic Regression F1 Score:\n",
      " 0.7047402208518146\n"
     ]
    }
   ],
   "source": [
    "                        # Importing needed packages\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import requests\n",
    "import io\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "    \n",
    "# Training csv file\n",
    "url_train = \"https://raw.githubusercontent.com/SoniaLei/nlp-web-scrapping/development/data/raw/tweets-train.csv\"\n",
    "csv_train = requests.get(url_train).content\n",
    "df_train = pd.read_csv(io.StringIO(csv_train.decode('utf-8')))\n",
    "\n",
    "# Testing csv file\n",
    "url_test = \"https://raw.githubusercontent.com/SoniaLei/nlp-web-scrapping/development/data/raw/tweets-test.csv\"\n",
    "csv_test = requests.get(url_test).content\n",
    "df_test = pd.read_csv(io.StringIO(csv_test.decode('utf-8')))\n",
    "\n",
    "X_train = df_train['text'].astype(str)\n",
    "Y_train = df_train['sentiment'].astype(str)\n",
    "\n",
    "X_test = df_test['text'].astype(str)\n",
    "Y_test = df_test['sentiment'].astype(str)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom cleaner function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Bag-of-Words Vectoriser\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = text_cleaner)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Building OVR Logisitic Regression Classifier\n",
    "\n",
    "LogReg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "ovr = OneVsRestClassifier(LogReg)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector)\n",
    "                 ,('classifier', ovr)])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"RAW DATA - TF-IDF:\\n\")\n",
    "print(\"Logistic Regression Accuracy:\\n\",metrics.accuracy_score(Y_test, predicted),\"\\n\") # Accuracy\n",
    "print(\"Logistic Regression Precision:\\n\",metrics.precision_score(Y_test, predicted, average='macro'),\"\\n\") # Precision\n",
    "print(\"Logistic Regression Recall:\\n\",metrics.recall_score(Y_test, predicted, average='macro'),\"\\n\") # Recall\n",
    "print(\"Logistic Regression F1 Score:\\n\",metrics.f1_score(Y_test, predicted, average='macro')) # F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# TF-IDF Vectoriser\n",
    "## K-Fold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATA - TF-IDF:\n",
      "\n",
      "[0.68146262 0.68431587 0.69341339 0.68922853 0.69286754]\n"
     ]
    }
   ],
   "source": [
    "                        # Importing needed packages\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "    \n",
    "url = \"https://raw.githubusercontent.com/SoniaLei/nlp-web-scrapping/development/data/raw/tweets-train.csv\"\n",
    "csv = requests.get(url).content\n",
    "df = pd.read_csv(io.StringIO(csv.decode('utf-8')))\n",
    "\n",
    "X = df['text'].astype(str)\n",
    "Y = df['sentiment'].astype(str)\n",
    "\n",
    "X_test = df_test['text'].astype(str)\n",
    "Y_test = df_test['sentiment'].astype(str)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom cleaner function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Bag-of-Words Vectoriser\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = text_cleaner)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Building OVR Logisitic Regression Classifier\n",
    "\n",
    "LogReg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "ovr = OneVsRestClassifier(LogReg)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector)\n",
    "                 ,('classifier', ovr)])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "print(\"RAW DATA - TF-IDF:\\n\")\n",
    "print(cross_val_score(estimator=pipe, X=X, y=Y, cv=k_fold, scoring='accuracy', n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
