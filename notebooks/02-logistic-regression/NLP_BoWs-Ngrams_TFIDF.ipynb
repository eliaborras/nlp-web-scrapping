{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> NLP - Bag of Words, spaCy </center></h1>\n",
    "\n",
    "### From:\n",
    "- https://medium.com/analytics-vidhya/fundamentals-of-bag-of-words-and-tf-idf-9846d301ff22\n",
    "- https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af\n",
    "\n",
    "***Linked with 'NLP_Logistic-Regression.ipynb' file***\n",
    "\n",
    "## *Written by Nathanael Hitch*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "Bag-of-Words (BoW) is a popular, simple method of feature extraction with text data.<br>\n",
    "The problem with modeling text is that it is messy; techniques like machine learning algorithms prefer well defined, fixed-length inputs and outputs.<br>\n",
    "Machine learning algorithms can't work with raw text directly; the text must be converted into numbers, specifically, vectors of numbers.\n",
    "\n",
    "Simply put: similar text must result in closer vector.\n",
    "\n",
    "In this model, text (sentence or a document) is represented as the bag of words; it disregards grammar and word order but keeps multiplicity (the number of times a word appears in the 'bag'). BoW models is commonly used in document classification methods, where the frequency of each word is used as a feature for training a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model involves 2 things;\n",
    "\n",
    "**1. A vocabulary of known words:**<br>\n",
    "constructing a document corpus (a collection of written texts), consisting of unique words in the whole text. Similar to a dictionary with each index corresponding to word.\n",
    "\n",
    "Example: 4 reviews for an Italian pasta dish:\n",
    "\n",
    "- This pasta is vert tasty and affordable.\n",
    "- This pasta is not tasty and is affordable.\n",
    "- This past is delicious and cheap.\n",
    "- Pasta is tasty and pasta tastes good.\n",
    "\n",
    "Counting: there are 12 unique words.\n",
    "\n",
    "**2. A measure of the presense of known words**<br>\n",
    "Taking the first review, we can plot the count of each unique word for each review.\n",
    "\n",
    "<img src=\"Images\\Unique_words_count.png\">\n",
    "\n",
    "<center>Top row = index for a unique word; Second row = the count for that word.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "Converting these reviews into vectors means that we can compare difference sentences, calculating the *Euclidean distance* between them.<br>\n",
    "This checks how similar 2 sentences are; no common words means a much larger distance.\n",
    "\n",
    "BoW doesn't work well with small changes in terminology, the reviews all have similar meanings just different words, resulting in a vector with a lot of zeros, a sparse vector. These require more memory and resources when modeling.<br>\n",
    "Hence it is better to decrease the size of the vocabulary when using BoW models, i.e. clean up the text:\n",
    "\n",
    "- ignore case\n",
    "- ignore punctuation\n",
    "- remove stopwords\n",
    "- fix misspelt words\n",
    "- reduce words to their stem, Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams Model:\n",
    "\n",
    "These are more sophisticated as they create a vocabulary of grouped words, allowing BoW to capture more meaning from the document with each word or token a 'gram'.\n",
    "\n",
    "\"*An N-gram is an N-token sequence of words: a 2-gram (or 'bigram') is a two-word sequence of words like “please turn”, “turn your”, or “your homework”. A 3-gram (more commonly called a trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”\"*.\n",
    "\n",
    "## Example use of Bag of Words and N-grams\n",
    "\n",
    "To build a representation of our vocabulary, we will use a Vectorizer to convert them into a numerical structure. Each vectoriser will:\n",
    "\n",
    "- Split the document into tokens.\n",
    "- Assign a weight to each token proportional to the frequency with which it shows up in the document.\n",
    "- Creating a document-term matrix: row = a document, column = a token.\n",
    "\n",
    "There are 3 common types of vectorisers:\n",
    "\n",
    "- CountVectorizer: most straightforward, it counts the number of times a token appears in the document using its value as a weight.\n",
    "\n",
    "- HashVectorizer: designed to be as memory efficient as possible; the vectorizer applies the hashing trick to encode tokens as numerical indexes rather than strings. The downside is that once vectorised, the features’ names can no longer be retrieved.\n",
    "\n",
    "- TF-IDF Vectorizer: *more on this below*; the weight assigned to each token depends on its frequency in a document, but also how recurrent that term is in all the documents.\n",
    "\n",
    "From: *https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools*\n",
    "\n",
    "The example below, from neptune.ai (above link), also has a few charts to show its counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b523f3f688>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "news = pd.read_csv('Files/abcnews-date-text.csv',nrows=10000)\n",
    "# Read .csv file\n",
    "\n",
    "def get_top_ngram(corpus, n=None):\n",
    "# corpus = collection of documents - n = the size of the n-gram (2 = bigram)\n",
    "    \n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    \n",
    "    \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) \n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]\n",
    "\n",
    "top_n_bigrams=get_top_ngram(news['headline_text'],2)[:10]\n",
    "x,y=map(list,zip(*top_n_bigrams))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the news headlines, *Bigrams* such as 'anti war' and 'killed in', that are related to war, dominate.\n",
    "\n",
    "### Customise Vectoriser<br>https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af\n",
    "\n",
    "While each vectoriser comes with its own preprocesor, tokeniser and analyser, you can assign a custom one if needed:\n",
    "\n",
    "- build_preprocessor: used to preprocess the input text before tokenization.\n",
    "- build_tokenizer: a function that splits a document into tokens.\n",
    "- build_analyzer: a function that applies preprocessing, tokenisation, removes stopwords and creates n-grams.\n",
    "\n",
    "Example below - function won't actually work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vec = CountVectorizer(preprocessor=my_preprocessor,\n",
    "                             tokenizer=my_tokenizer,\n",
    "                             ngram_range=(1,2),\n",
    "                             stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E.g.** the code below uses a custom function (spacy_cleaner) that  tokenises but also lemmatisises and puts the words into lower case. With words in their base form (testing/tested -> test), it makes classification of documents easier.\n",
    "\n",
    "`The 'spacy_cleaner function (below) was taken from 'NLP_Logistic-Regression.ipynb'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:\n",
      " This is a test sentence, for testing tests from London. \n",
      "\n",
      "Sentence tokenised:\n",
      " [This, is, a, test, sentence, ,, for, testing, tests, from, London, .] \n",
      "\n",
      "Sentence without stopwords or punctuations:\n",
      " [test, sentence, testing, tests, London] \n",
      "\n",
      "Sentence lemmatisted, no spaces and lowercase (except Proper Noun):\n",
      " ['test', 'sentence', 'test', 'test', 'London'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test', 'sentence', 'test', 'test', 'London']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string # Contains a useful list of punctuation marks.\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "############################### Tokeniser for BoW ###############################\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Create list of punctuation marks\n",
    "\n",
    "def spacy_cleaner(sentence):\n",
    "    \n",
    "    print(\"Input sentence:\\n\", sentence,\"\\n\")\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    # Pass text into model's pipeline.\n",
    "    \n",
    "    myTokens = [token for token in doc]\n",
    "    # Creating a list of the words in the sentence.\n",
    "    print(\"Sentence tokenised:\\n\", myTokens,\"\\n\")\n",
    "    \n",
    "    myTokens = [token for token in myTokens if token.is_stop == False and token.text not in punctuations]\n",
    "    # List of words without stopwords or punctuations.\n",
    "    print(\"Sentence without stopwords or punctuations:\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    myTokens = [token.lemma_.strip().lower() if token.pos_ != \"PROPN\" else token.lemma_.strip() \\\n",
    "                for token in myTokens]\n",
    "    # Words are lemmatised, spaces at end removed and (if not a proper noun) lowercased.\n",
    "    \n",
    "    print(\"Sentence lemmatisted, no spaces and lowercase (except Proper Noun):\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    return myTokens\n",
    "    \n",
    "spacy_cleaner(\"This is a test sentence, for testing tests from London.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Vectoriser using BoW #############################\n",
    "\n",
    "# We are using the 'spacy_cleaner' function as the tokeniser\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_cleaner, ngram_range=(1,1))\n",
    "# Bag-of-Words n-gram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF - Term Frequency - Inverse Document Frequency\n",
    "\n",
    "TF-IDF is a \"*numerical statistic intended to show how important a word is in a document in a collection or corpus*\".<br>\n",
    "The concept **counts** the number of times the word appears in a document and figures out the **frequency** that the word ($w_i$) appears in a document out of all the words in the document ($r_i$):\n",
    "\n",
    "\\begin{equation}\n",
    "TF(w_i,r_i) = \\frac{No.-of-times-w_i-occurs-in-r_i}{Total-no.-of-words-in-r_i}\n",
    "\\end{equation}\n",
    "\n",
    "TF is basically the *probability of finding a word in a document*.\n",
    "\n",
    "The TF-IDF value:\n",
    "\n",
    "- increases proportional to the number of times it appears in the document.\n",
    "- is offset by the number of documents in the corpus that contain the word.\n",
    "\n",
    "Th Inverse Document Frequency is a *measure of how much information the word provides*, if it's common or rare across ALL documents. Words that occur rarely over the documents have a high IDF score. It is logarithmically scaled:\n",
    "\n",
    "\\begin{equation}\n",
    "IDF(w_i,D) = \\log{\\frac{N}{|{d \\in D:w_i \\in d}|}} = \\log{\\frac{Total-No.-of-docs}{No.-of-docs-containing-w_i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual **TF-IDF** value is simply TF multiplied by IDF:\n",
    "\n",
    "<center>$TF(w_i,r_i) = IDF(w_i,D)$</center>\n",
    "\n",
    "A high weight in tf–idf is reached by a high frequency of the term in the given document, but a low document frequency of the term in the whole collection of documents. Hence, the weights hence tend to filter out common terms.\n",
    "\n",
    "**Fun Maths**: the fraction inside IDF's log function is always $\\ge$ 1, hence the value of IDF, and TF-IDF, is $\\ge$ 0. As a term appears in more documents, it brings the IDF and tf–idf closer to 0.\n",
    "\n",
    "**Basically:**\n",
    "***\n",
    "- `It's a way of representing how important a particular term is in the context of a given document, based on how many times the term appears and how many other documents that same term appears in.`\n",
    "-`The higher the TF-IDF, the more important that term is to that document.`\n",
    "- `TF-IDF gives larger values for less frequent words in a collection (corpus) of documents`\n",
    "- `A high value occurs when both TF and IDF values are high: the word is rare in the collection but frequent in a document`\n",
    "- `However, TF-IDF does not take the semantic meaning of the words`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_cleaner)\n",
    "# TF-IDF result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**Both of these forms of Bag-of-Words can be used in the pipeline for a model.<br>\n",
    "*E.g. NLP_Logisitic-Regression.ipynb***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
