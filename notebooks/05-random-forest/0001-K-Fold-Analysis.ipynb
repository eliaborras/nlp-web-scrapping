{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> K-Fold Analysis </center></h1>\n",
    "\n",
    "LinkedIn Learning: https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/cross-validation-and-evaluation-metrics?u=78163626<br>\n",
    "https://machinelearningmastery.com/k-fold-cross-validation/<br>\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "## *Written by Nathanael Hitch*\n",
    "\n",
    "<div style=\"color:purple\">Background in classification and other metric in:</div>\n",
    "<ul style=\"color:purple\">\n",
    "    <li>NLP_Logisitic-Regression.ipynb</li>\n",
    "    <li>Random_Forest.ipynb</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous looks at NLP sentiment models, there have been ways of analysing how good a model is when using testing data. For example:\n",
    "\n",
    "- Accuracy: percentage of the total predictions that are completely correct.\n",
    "\n",
    "# What is it?\n",
    "\n",
    "It is when \"the full data set is divided into *k*-subsets (*k* being a number) with the holdout method repeated *k* times. Each time, one of the subsets is used to test the model while all the other subsets are put together to train the model\".\n",
    "\n",
    "For a **5 fold validation**, k = 5, with a test set of 10,000 examples:\n",
    "\n",
    "1. The test set is split into **5** subsets.\n",
    "2. The first subset is put aside to test; the other 4 subsets train the model.\n",
    "3. The accuracy (metric) for the model is determined by using the testing subset.\n",
    "4. Each subset is used as a testing subset, with the other 4 training the model to be tested.\n",
    "5. Each time the accuracy score is attained from each model.\n",
    "6. The full array of avergae scores is output, along with the average of the model's accuracy.\n",
    "\n",
    "You can see what difference between the lowest and highest score compared with the model's average score.<br>\n",
    "Be aware that even small drops in accuracy can cause big changes in the results, e.g. a business setting where there are millions of pounds involved.\n",
    "\n",
    "## Usefulness\n",
    "\n",
    "Testing the model over a number of differenct sets will give a better indication about how the model will perform in the real-world.<br>\n",
    "Depending on how big the differences between the averages are, we understand how well the model will work when put into production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sklearn to import in the K_Fold API for ease of use. Using the KFold object, you can see the split in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.1 0.4 0.5 0.6], test: [0.2 0.3]\n",
      "train: [0.2 0.3 0.4 0.6], test: [0.1 0.5]\n",
      "train: [0.1 0.2 0.3 0.5], test: [0.4 0.6]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
    "# data sample\n",
    "\n",
    "kfold = KFold(3, True, 1)\n",
    "# prepare cross validation\n",
    "\n",
    "for train, test in kfold.split(data):\n",
    "    print('train: %s, test: %s' % (data[train], data[test]))\n",
    "# enumerate splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - sentiment analysis\n",
    "\n",
    "We first need to upload the data needed; we can upload a csv file with the needed information (using pandas).<br>\n",
    "The necessary columns will then be separated into variables for the testing and training of the model:\n",
    "\n",
    "- body_text = text to be analysed\n",
    "- sentiment = sentiment of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data = pd.read_csv(\"test_data.csv\")\n",
    "# csv data file\n",
    "\n",
    "X_data = df['body_text']\n",
    "# Text to be analysed\n",
    "\n",
    "Y_labels = df['sentiment']\n",
    "# Sentiment of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be code to tokenise the text, as well as additionally to clean it.\n",
    "\n",
    "The k-fold analysis is then set up using sklearn modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# n_splits is the number of splits of the data needed, 5 in this case\n",
    "\n",
    "cross_val_score(estimator=rfc, X=X, y=Y_labels, cv=k_fold, scoring='accuracy', n_jobs=-1)\n",
    "# Cross Validating the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are number of attributes that can be used with 'cross_val_score'. The couple used above are the necessary ones:\n",
    "\n",
    "- estimator: **which classifier are you using?**<br>\n",
    "In this example, it is the Random Forest Classifier; however other classifiers can be used, such as Logistic Regression, SVM or Bayesian.\n",
    "\n",
    "- X: **the variable that is being analysed.**<br>\n",
    "E.g. the 'body_text' data\n",
    "\n",
    "- y: **what the sentiment of the text should be.**<br>\n",
    "E.g. the 'sentiment' data\n",
    "\n",
    "- cv: **the cross-validation splitting strategy.**<br>\n",
    "Input the cross-validation generator (as above), or an iterable for how many k-folds are wanted.\n",
    "\n",
    "- scoring: **what metric do you want analysed in the models?**<br>\n",
    "This can be accuracy, precision, recall etc.\n",
    "\n",
    "- n_jobs = -1: **models to be made in parallel.**<br>\n",
    "Models made independent of eachother and are trained and tested more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Code\n",
    "\n",
    "Below is a complete code for k-fold analysis for a Random Forest Regressor using a Bag-of-Words vectoriser, with some additional code for cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69365108 0.69505095 0.69359534 0.68941048 0.69978166]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import winsound\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "df = pd.read_csv(\"Files/raw/tweets-train.csv\")\n",
    "\n",
    "X = df['text'].astype(str)\n",
    "# Convert to type 'string' as pandas converts inputs to their most relevant type\n",
    "    # The issue is sometimes pandas converts data to a 'float'; this doesn't work with the evaluation functions\n",
    "\n",
    "Y = df['sentiment'].astype(str)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom tokeniser and cleaning function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens    \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Vectoriser and Classifier\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = text_cleaner, ngram_range=(1,1))\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', rfc)])\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "print(cross_val_score(estimator=pipe, X=X, y=Y, cv=k_fold, scoring='accuracy', n_jobs=-1))\n",
    "\n",
    "winsound.PlaySound(\"Files/Alarm07.wav\", winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
