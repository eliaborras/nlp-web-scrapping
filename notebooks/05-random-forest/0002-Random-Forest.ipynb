{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Random Forest Classification </center></h1>\n",
    "\n",
    "LinkedIn Learning: https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/introducing-random-forest?u=78163626<br>\n",
    "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "\n",
    "## *Written by Nathanael Hitch*\n",
    "\n",
    "<span style=\"color:OrangeRed\">For background in classification - 'NLP_Logisitic-Regression.ipynb'</span>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<span style=\"background-color:DeepPink; color:white; font-size:20px\">Appends Added:</span>\n",
    "\n",
    "1. An example of a multiple class dataset\n",
    "2. Using grid search\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before - Ensemble Method\n",
    "\n",
    "**Ensemble method** is a technique that \"creates multiple models and combines them to produce better results than any single model\"/<br>\n",
    "This helps the model to depend on aggregated opinions of many models rather than the individual opinion of one model.\n",
    "\n",
    "# What is it?\n",
    "\n",
    "**Randon Forest** is an \"Ensemble learning method that constructs a collection of decision trees and then aggregates the predictions of each tree to determine the final prediction\".<br>\n",
    "The individual decision trees are the 'weak models' that are combined into the aggreagated Forest model.\n",
    "\n",
    "In a binary classification (positive or negative), each individually built decision tree will give a result of positive or negative with the aggregated result being the classification with the most results (positive = 60, negative = 40, positive is the aggregate result).<br>\n",
    "Simply put, it's a simple voting method for the trees.\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- Considered highly accurate and robust due to the number of decision trees in the process.\n",
    "- Can be used for classification (categorical) or regression (continous).\n",
    "- Easily handles outliers, missing values (median values or weighted averages), scewed data or data that's not on the same scale.\n",
    "- Accepts various types of inputs (continous, ordinal/statistical, etc.).\n",
    "- Less likely to Overfit as the average of the decision trees cancels out the biases.\n",
    "- Outputs feature importance.\n",
    "\n",
    "Useful as data cleaning isn't necessarily needed as it accepts anything.\n",
    "\n",
    "<span style=\"color:white; background-color:DarkBlue\">As it can be used in regression, as well as classification, it can handle multiple class data sets.</span>\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- Slow to generate results due to multiple decision trees.\n",
    "- Model difficult to interpret compared to a decision tree.\n",
    "\n",
    "# Example\n",
    "\n",
    "## Set Up\n",
    "\n",
    "This starts very similar to Logistic Regression:\n",
    "\n",
    "- read in file\n",
    "- tokeniser and clean the data<br>\n",
    "In this case, tokenise, remove stop words and punctuation and lemmatise.<br>\n",
    "- create vectoriser: Count or Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # Importing needed packages\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import string\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "#df = pd.read_csv(\"Files/SMSSpamCollection.tsv\", sep='\\t')\n",
    "\n",
    "df = pd.read_csv(\"Files/SMS_test.tsv\", sep='\\t')\n",
    "# Smaller testing file\n",
    "\n",
    "df.columns = ['label','body_test']\n",
    "# File doesn't have column headers - now they have names assigned to their columns\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom tokeniser and cleaning function\n",
    "\n",
    "def spacy_cleaner(sentence):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    punctuations = string.punctuation\n",
    "    \n",
    "    #print(\"Input sentence:\\n\", sentence,\"\\n\")\n",
    "    \n",
    "    doc = nlp(sentence.strip())\n",
    "    # Pass text into model's pipeline.\n",
    "    \n",
    "    myTokens = [token for token in doc]\n",
    "    # Creating a list of the words in the sentence.\n",
    "    #print(\"Sentence tokenised:\\n\", myTokens,\"\\n\")\n",
    "    \n",
    "    myTokens = [token for token in myTokens if token.is_stop == False and token.text not in punctuations]\n",
    "    # List of words without stopwords or punctuations.\n",
    "    #print(\"Sentence without stopwords or punctuations:\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    myTokens = [token.lemma_.strip().lower() if token.pos_ != \"PROPN\" else token.lemma_.strip() \\\n",
    "                for token in myTokens]\n",
    "    # Words are lemmatised, spaces at end removed and (if not a proper noun) lowercased.\n",
    "    \n",
    "    myTokens = [token for token in myTokens if token != \"\"]\n",
    "    \n",
    "    #print(\"Sentence lemmatisted, no spaces and lowercase (except Proper Noun):\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    return myTokens\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Vectoriser\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_cleaner, ngram_range=(1,1))\n",
    "\n",
    "#tfidf_vector = TfidfVectorizer(tokenizer = spacy_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Random Forest Classifier\n",
    "\n",
    "Next we need to create the Random Forest object; to see the list of all the attributes and methods for the Random Forest Classifier, use *dir(RandomForestClassifier)*.<br>\n",
    "By printing *RandomForestClassifier()* it will also show all the hyper-primaries plus their default settings.\n",
    "\n",
    "This object has n_jobs = -1 as it'll allow the decision trees to be made independent of one another, in parallel, so that they can be made faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_estimator_type', '_get_param_names', '_get_tags', '_make_estimator', '_more_tags', '_required_parameters', '_set_oob_score', '_validate_X_predict', '_validate_estimator', '_validate_y_class_weight', 'apply', 'base_estimator', 'bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'decision_path', 'estimator_params', 'feature_importances_', 'fit', 'get_params', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'predict', 'predict_log_proba', 'predict_proba', 'random_state', 'score', 'set_params', 'verbose', 'warm_start'] \n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=4, n_jobs=-1,\n",
      "                       oob_score=False, random_state=None, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=4)\n",
    "# n_jobs allows the decision trees to be made in parallel\n",
    "    # The model is trained and tested more quickly.\n",
    "\n",
    "print(dir(rfc),\"\\n\")\n",
    "# Lists the classes Attributes and Methods\n",
    "\n",
    "print(rfc)\n",
    "# Shows the Hyper-primaries and their default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amongst a lot of hyper-primaries, *I will not be going through every one*, there is **n_estimators**. This is the number of decision trees that will be buld within the model; at the moment there will be '10' trees built with unlimited steps. There will then be a vote amongst the trees to make the final decision.\n",
    "\n",
    "Also of significance is **max_depth** which is \"*how deep each of the decision trees is*\". It's default is 'None' which means that the model build each decision tree until it minimises some loss criteria.<br>\n",
    "<span style=\"background-color:red; color:white\">I'll be honest, I'm not completely sure what this means.</span>\n",
    "\n",
    "*As a note*, Random Forest is built on relatively few fully built decision trees; this will become apparent later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "### Standard\n",
    "\n",
    "One way is to split the data into training and testing groups and train and test the model that way.\n",
    "\n",
    "<span style=\"background-color:red; color:white\">NOTE:</span>\n",
    "`For this way, the data has been reduced and the number of decision trees has been lessened, otherwise it takes the computer a long time to process the information.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:\n",
      " 0.9066666666666666 \n",
      "\n",
      "Random Forest Precision:\n",
      " 0.9507042253521127 \n",
      "\n",
      "Random Forest Recall:\n",
      " 0.6818181818181819 \n",
      "\n",
      "Random Forest F1 Score:\n",
      " 0.7407407407407407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Splitting the test and training\n",
    "    \n",
    "X = df['body_test']\n",
    "Y = df['label']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Building the model\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', rfc)])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Random Forest Accuracy:\\n\",metrics.accuracy_score(Y_test, predicted),\"\\n\") # Accuracy\n",
    "print(\"Random Forest Precision:\\n\",metrics.precision_score(Y_test, predicted, average='macro'),\"\\n\") # Precision\n",
    "print(\"Random Forest Recall:\\n\",metrics.recall_score(Y_test, predicted, average='macro'),\"\\n\") # Recall\n",
    "print(\"Random Forest F1 Score:\\n\",metrics.f1_score(Y_test, predicted, average='macro')) # F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold\n",
    "\n",
    "<center><span style=\"background-color:red; color:white; font-size:30px\">DO NOT RUN BELOW: this way is very slow, taking at least half an hour to process!!</span></center>\n",
    "\n",
    "`In order for this to work, we need to use the full data set not the reduced set.`\n",
    "\n",
    "Firstly, the testing text (df['body_test']) needs to be transformed and input into the vectoriser (Bag-of-Words or TF-IDF) so that it can be input into the Cross Validation Score object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = bow_vector.fit_transform(df['body_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span style=\"background-color:chartreuse\">This way does work but, as show by the big red signs, it is terribly slow! This is due to the data being uploaded to the spaCy model, which slows everything down. The LinkedIn example (shown later) has a better way which we will use later.</span></center>\n",
    "\n",
    "<center><span style=\"background-color:red; color:white; font-size:30px\">DO NOT RUN ABOVE: this way is very slow, taking at least half an hour to process!!</span></center>\n",
    "\n",
    "`The Random Forest classifier will be generated again for this part.`<br>\n",
    "With the Random Forest classifier, the K-Fold object will be generated next with the amount of folds, *K*, stated.\n",
    "\n",
    "Lastly, generate the 'Cross Validation Score' object to put all the components together to return a score. The cross_val_score object needs to know:\n",
    "\n",
    "- The model we're using = generate new one for this part\n",
    "- Input features = **X_features**\n",
    "- The label, i.e. the answer of the text = **df['label']** or **Y**\n",
    "- How we're splitting the data (cv) = **k_fold**\n",
    "- What scoring metric we use (scoring) =  in this case **accuracy**, another metric can be chosen\n",
    "- Best to put **n_jobs = -1** as each can be run independent of one another\n",
    "\n",
    "The output will be an array of showing the accuracy for each iteration of the model. In this case, **5** results should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9       , 0.9       , 0.93      , 0.87      , 0.92929293])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "cross_val_score(estimator=rfc, X=X_transform, y=df['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Example = much quicker\n",
    "\n",
    "As mentioned, the text_cleaner intially developed, before showing the LinkedIn example, uses a spaCy model. These have their advantages as they return tokenised objects which have methods that can be used directly, e.g. Lemmatisation (*lemma_*) or Stop Words (*is_stop*). On the other hand, NLTK returns string objects that have been affected by NLTK classes, like a Lemmatiser.<br>\n",
    "**However**, the spaCy model can slow down the model as the model is applied to each string that comes from the file.\n",
    "\n",
    "The LinkedIn example will be quickly shown and then a basic code has been written below as not all of the stuff in the LinkedIn example is necessary for the K-Fold analysis.\n",
    "\n",
    "<hr>\n",
    "<center><span style=\"color:white; background-color: blue; font-size:20px\">Refresher on Lambdas</span></center>\n",
    "\n",
    "https://www.w3schools.com/python/python_lambda.asp\n",
    "\n",
    "\"*A lambda function is a small anonymous function; it can take any number of arguments, but can only have one expression.*\"\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just a: 15 \n",
      "\n",
      "a and b: 30\n"
     ]
    }
   ],
   "source": [
    "x = lambda a : a + 10\n",
    "\n",
    "print(\"Just a:\",x(5),\"\\n\")\n",
    "\n",
    "x = lambda a, b : a * b\n",
    "\n",
    "print(\"a and b:\",x(5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda is better shown when you use them as an anonymous function inside another function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(n):\n",
    "    \n",
    "    return lambda a : a * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use that function in many different ways, even using the same function to make 2 different functions in the same programme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input: 10\n",
      "\n",
      "Double Function: 20 \n",
      "\n",
      "Triple Function: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Input: 10\\n\")\n",
    "\n",
    "myDoubler = myfunc(2)\n",
    "\n",
    "print(\"Double Function:\", myDoubler(10), \"\\n\")\n",
    "\n",
    "myTripler = myfunc(3)\n",
    "\n",
    "print(\"Triple Function:\", myTripler(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Below is how the **LinkedIn Learning** example used the for transforming the 'body_test' data, and then Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8094</th>\n",
       "      <th>8095</th>\n",
       "      <th>8096</th>\n",
       "      <th>8097</th>\n",
       "      <th>8098</th>\n",
       "      <th>8099</th>\n",
       "      <th>8100</th>\n",
       "      <th>8101</th>\n",
       "      <th>8102</th>\n",
       "      <th>8103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...  8094  8095  \\\n",
       "0       128     4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        49     4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2        62     3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        28     7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4       135     4.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   8096  8097  8098  8099  8100  8101  8102  8103  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "# Removes affixes from words - dies -> die - flies -> fli\n",
    "    # Similar to spacy's lemmatiser but ma not return actual words (as above)\n",
    "\n",
    "data = pd.read_csv(\"Files/SMSSpamCollection.tsv\", sep='\\t')\n",
    "# Reading the file\n",
    "\n",
    "data.columns = ['label', 'body_text']\n",
    "# Labeling the columns\n",
    "\n",
    "\n",
    "# Not sure this is necessary for the model\n",
    "def count_punct(text):\n",
    "\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    # Calculates the number of punctuation symbols in the text\n",
    "    \n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "    # Returns a percentage of the text that is punctucation symbols (excluding spaces)\n",
    "    \n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "# Number of characters in the text without the spaces\n",
    "\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "# Using the 'count_punct' function on the text\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    # Removing any punctuation characters\n",
    "        # The use of 'word' is a bit misleading\n",
    "    \n",
    "    tokens = re.split('\\W+', text)\n",
    "    # Splitting the words using regex\n",
    "    \n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    # Using 'Porter Stemmer' to remove word affixes\n",
    "        # Not quite as good as Lemmatisation\n",
    "    \n",
    "    return text\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "X_features = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97935368, 0.97935368, 0.97843666, 0.96585804, 0.97484277])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "cross_val_score(estimator=rfc, X=X_features, y=data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X_features has a couple of bits of extra information that isn't needed for the K-Fold analysis of a Random Forest classifier.\n",
    "\n",
    "### Simplified Example\n",
    "\n",
    "Below is a simpler cleaning function, compared to the LinkedIn example, but is quicker (MUCH quicker) compared to the first code.<br>\n",
    "The code is split up to make things easier; firstly the set up similar to other classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>8796</th>\n",
       "      <th>8797</th>\n",
       "      <th>8798</th>\n",
       "      <th>8799</th>\n",
       "      <th>8800</th>\n",
       "      <th>8801</th>\n",
       "      <th>8802</th>\n",
       "      <th>8803</th>\n",
       "      <th>8804</th>\n",
       "      <th>8805</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8806 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  8796  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   8797  8798  8799  8800  8801  8802  8803  8804  8805  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 8806 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import winsound\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "df = pd.read_csv(\"Files/SMSSpamCollection.tsv\", sep='\\t')\n",
    "\n",
    "df.columns = ['label','body_test']\n",
    "# File doesn't have column headers - now they have names assigned to their columns\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom tokeniser and cleaning function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens    \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Vectoriser\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = text_cleaner, ngram_range=(1,1))\n",
    "\n",
    "#tfidf_vector = TfidfVectorizer(tokenizer = text_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the LinkedIn code, we can show the results of the vectoriser. However, this is not necessary when developing the actual code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Transforming the data\n",
    "\n",
    "X_bow = bow_vector.fit_transform(df['body_test'])\n",
    "\n",
    "X_transform = pd.concat([pd.DataFrame(X_bow.toarray())], axis=1)\n",
    "\n",
    "winsound.PlaySound(\"Files/Alarm07.wav\", winsound.SND_FILENAME)\n",
    "\n",
    "X_transform.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard way (for myself anyway) is to use the Pipeline to vectorise and classify the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97935368 0.97576302 0.97484277 0.95867026 0.96585804]\n"
     ]
    }
   ],
   "source": [
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "    \n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', rfc)])\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "print(cross_val_score(estimator=pipe, X=df['body_test'], y=df['label'], cv=k_fold, scoring='accuracy', n_jobs=-1))\n",
    "\n",
    "winsound.PlaySound(\"Files/Alarm07.wav\", winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also use the vectorisation transformation so that the vectoriser isn't needed as it's already been used.<br>\n",
    "Either way, the same result is attained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With X_bow:\n",
      " [0.97845601 0.97666068 0.97663971 0.9640611  0.96495957]\n"
     ]
    }
   ],
   "source": [
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "    \n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "print(cross_val_score(estimator=rfc, X=X_bow, y=df['label'], cv=k_fold, scoring='accuracy', n_jobs=-1))\n",
    "\n",
    "winsound.PlaySound(\"Files/Alarm07.wav\", winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>APPEND 1</center></h1>\n",
    "\n",
    "# An example of a multiple class dataset\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "\n",
    "Using the Iris Flower dataset, <span style=\"color:red\">dataset not csv file</span>, built into the sklearn module we can show a more complex use of Random Forest.<br>\n",
    "The model will classify the data for each flower into one of three classes: setosa, vericolor and virginia. However, the dataset will use four different columns of data to make the classification.\n",
    "\n",
    "It's pretty simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flower classes:  ['setosa' 'versicolor' 'virginica'] \n",
      "\n",
      "Flower data columns:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# print the label species(setosa, versicolor, virginica)\n",
    "print(\"Flower classes: \", iris.target_names,\"\\n\")\n",
    "\n",
    "# print the names of the four features\n",
    "print(\"Flower data columns: \", iris.feature_names, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to seperate the dependent (features) and independent (labels) variables out.\n",
    "\n",
    "As we're using a dataset, we first need to convert it to a dataframe to use. As we will be using one training model, we will then use the train_test_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame of given iris dataset.\n",
    "data=pd.DataFrame({\n",
    "    'sepal length':iris.data[:,0],\n",
    "    'sepal width':iris.data[:,1],\n",
    "    'petal length':iris.data[:,2],\n",
    "    'petal width':iris.data[:,3],\n",
    "    'species':iris.target\n",
    "})\n",
    "\n",
    "X = data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\n",
    "y = data['species']  # Labels\n",
    "\n",
    "# For a csv file, X and y can be done the same way\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then .fit() the model to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=10, n_jobs=-1)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see what the accuracy of the model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can **find** and **visualise** important features of the daat using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "petal width (cm)     0.461067\n",
       "petal length (cm)    0.427307\n",
       "sepal length (cm)    0.087476\n",
       "sepal width (cm)     0.024150\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "feature_imp = pd.Series(clf.feature_importances_,index=iris.feature_names).sort_values(ascending=False)\n",
    "\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the numbers and the graph, the 'sepal width' is of low importance. This means it can be removed from the features, leaving 'X' with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['petal length', 'petal width','sepal length']]  # Removed feature \"sepal length\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>APPEND 2</center></h1>\n",
    "\n",
    "# Using grid search\n",
    "\n",
    "Able to vary a set of hyper-variables, making a 'grid', to look at results between classifiers with varying hyper-variables to see what values for the hyper-variables give the best results for the model.\n",
    "\n",
    "We will start with setting up the data and the function to clean the data as normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "df = pd.read_csv(\"Files/SMSSpamCollection.tsv\", sep='\\t')\n",
    "\n",
    "df.columns = ['label','body_test']\n",
    "# File doesn't have column headers - now they have names assigned to their columns\n",
    "\n",
    "X = df['body_test'] #.astype(str)\n",
    "\n",
    "y_labels = df['label'] #.astype(str)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom tokeniser and cleaning function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(analyzer=text_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this grid, we will be varying the n_estimators (how many decision trees in each model, default = 10) and max_depth (how 'deep' each tree goes, default = none, i.e. no limit to the depth).\n",
    "\n",
    "We next need to split the data between training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will write a function that will accept the estimators and model's depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def train_RF(n_est, depth):\n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    # Creating a classifier that will be created will specified hyper-variables\n",
    "    \n",
    "    pipe = Pipeline([('vectorizer', tfidf_vector)\n",
    "                    ,('classifier', rfc)])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    # Train that model\n",
    "    \n",
    "    predicted = pipe.predict(X_test)\n",
    "    # Test that model\n",
    "    \n",
    "    precision, recall, fscore, support = score(y_test, predicted, pos_label = 'spam', average='binary')\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    # Getting the metrics from that model\n",
    "        # pos_label = what label is considered the positive one\n",
    "        \n",
    "    print('Est: {} / Depth: {} ----- Precision: {} / Recall: {} / Metrics_Accuracy: {} / Calculated_Accuracy: {}'.format(\n",
    "        n_est, depth, round(precision, 3), round(recall, 3), round(accuracy, 3), round((predicted==y_test).sum() / len(predicted), 3)))\n",
    "    # Print out metrics to compare each model\n",
    "    \n",
    "        # The Calculated Accuracy:\n",
    "            # predicted==y_test pairs these 2 together, returning a boolean list of whether they're equal to eachother\n",
    "            # The no. of correct predicted values divided by the number of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the code to show the grid functionality of the models.<br>\n",
    "This will loop through different combinations of values for the variables to show the precision, recall, and accuracy values for each model as a comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Depth: 10 ----- Precision: 1.0 / Recall: 0.178 / Metrics_Accuracy: 0.898 / Calculated_Accuracy: 0.898\n",
      "Est: 10 / Depth: 20 ----- Precision: 1.0 / Recall: 0.529 / Metrics_Accuracy: 0.941 / Calculated_Accuracy: 0.941\n",
      "Est: 10 / Depth: 30 ----- Precision: 1.0 / Recall: 0.673 / Metrics_Accuracy: 0.959 / Calculated_Accuracy: 0.959\n",
      "Est: 10 / Depth: None ----- Precision: 0.98 / Recall: 0.716 / Metrics_Accuracy: 0.963 / Calculated_Accuracy: 0.963\n",
      "Est: 50 / Depth: 10 ----- Precision: 1.0 / Recall: 0.279 / Metrics_Accuracy: 0.91 / Calculated_Accuracy: 0.91\n",
      "Est: 50 / Depth: 20 ----- Precision: 1.0 / Recall: 0.587 / Metrics_Accuracy: 0.949 / Calculated_Accuracy: 0.949\n",
      "Est: 50 / Depth: 30 ----- Precision: 1.0 / Recall: 0.673 / Metrics_Accuracy: 0.959 / Calculated_Accuracy: 0.959\n",
      "Est: 50 / Depth: None ----- Precision: 0.994 / Recall: 0.769 / Metrics_Accuracy: 0.971 / Calculated_Accuracy: 0.971\n",
      "Est: 100 / Depth: 10 ----- Precision: 1.0 / Recall: 0.231 / Metrics_Accuracy: 0.904 / Calculated_Accuracy: 0.904\n",
      "Est: 100 / Depth: 20 ----- Precision: 1.0 / Recall: 0.553 / Metrics_Accuracy: 0.944 / Calculated_Accuracy: 0.944\n",
      "Est: 100 / Depth: 30 ----- Precision: 1.0 / Recall: 0.663 / Metrics_Accuracy: 0.958 / Calculated_Accuracy: 0.958\n",
      "Est: 100 / Depth: None ----- Precision: 0.994 / Recall: 0.769 / Metrics_Accuracy: 0.971 / Calculated_Accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "for n_est in[10, 50, 100]:\n",
    "# Go through values 10, 50, 100 for estimators\n",
    "    \n",
    "    for depth in [10, 20, 30, None]:\n",
    "    # Go through values 10, 20, 30, None for depth\n",
    "        \n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compare the different values for each model:<br>\n",
    "Estimators, while make a small difference, don't make as big a difference as the depth of the model. Models with limited max_depths can be eliminated as they have low metric scores; it's probably best for the model to have no limit.\n",
    "\n",
    "This exercise is very broad: probably be expected to explore a wider range of hyper-variables and a wider range of their values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
