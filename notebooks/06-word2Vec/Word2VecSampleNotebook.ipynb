{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6tppNDl109O9QVucdQR8r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoniaLei/nlp-web-scrapping/blob/development/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bywoy84-5d5Y",
        "colab_type": "text"
      },
      "source": [
        " #### This is a word2vec notebook for word2vec model to represent distributed representations of words in a corpus C.\n",
        "\n",
        "\n",
        "In this model ,given a set of sentence, there are 2 approaches to this algorithim, namely:\n",
        "\n",
        "1) **Skip-Gram** : the model loops on the words of each sentence and tries to use the current word to predict its neighbours context\n",
        "\n",
        "2) **Continuous Bag Of Words**: uses each of the contexts to predict the current\n",
        "![alt text](https://miro.medium.com/max/1114/1*99ewFdV861rqY_K14bc69w.png)\n",
        "\n",
        "Word2vec is like an autoencoder, encoding each word and train againsts other words that neightbour them in the input corpus.\n",
        "\n",
        "\n",
        "In situation where the feature vector assigned to a word cannot be used accurately predict word's context, the components of the vector are adjusted.\n",
        "\n",
        "To limit the number of words in each content, a parameter \"window size\" is used.\n",
        "\n",
        "**Model overview**\n",
        "1. Creating a vocabulary of all the words in text and then to encode word as a vector of the same dimensionsof the vocabulary.\n",
        "2. Each dimension can be thought as a word in the vocabulary.\n",
        "3. Resulting a vector with all 0s and 1s representing the corresponding word in the vocabulary. (technique name as one-hot encoding)\n",
        "\n",
        "Skip-gram model creates a hot-vector for each word. \n",
        "A hot vector is a vector representation of a word where the vector is the size of the vocabulary (total unique words)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEhKoSqvKY5G",
        "colab_type": "text"
      },
      "source": [
        "### First of all we need to generate training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id0yEhO0D5Y0",
        "colab_type": "code",
        "outputId": "e0e6a1b7-7111-40fa-acb8-ea1c915ebc63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import re\n",
        "import numpy as npy\n",
        "\n",
        "def tokenize(text):\n",
        "  pattern = re.compile(r'[A-za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
        "  return pattern.findall(text.lower())\n",
        "\n",
        "def mapping(tokens):\n",
        "  word_to_id= dict()\n",
        "  id_to_word = dict()\n",
        "  for i, token in enumerate(set(tokens)):\n",
        "    word_to_id[token] = i\n",
        "    id_to_word[i] = token\n",
        "\n",
        "  return word_to_id, id_to_word\n",
        "\n",
        "def generate_training_data(tokens, word_to_id, window_size):\n",
        "  N = len(tokens)\n",
        "  X, Y = [], []\n",
        "\n",
        "  for i in range(N):\n",
        "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
        "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
        "        for j in nbr_inds:\n",
        "            X.append(word_to_id[tokens[i]])\n",
        "            Y.append(word_to_id[tokens[j]])\n",
        "\n",
        "  X = npy.array(X)\n",
        "  X = npy.expand_dims(X, axis=0)\n",
        "  Y = npy.array(Y)\n",
        "  Y = npy.expand_dims(Y, axis=0)\n",
        "  return X,Y\n",
        "\n",
        "corpus = \"After the deduction of the cost of investing,\" \\\n",
        "        \"beating the stock market is a loser's game.\"\n",
        "\n",
        "tokens = tokenize(corpus)\n",
        "word_to_id, id_to_word = mapping(tokens)\n",
        "\n",
        "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
        "vocab_size = len(id_to_word)\n",
        "m = Y.shape[1]\n",
        "\n",
        "Y_one_hot = npy.zeros((vocab_size,m))\n",
        "Y_one_hot[Y.flatten(), npy.arange(m)] = 1\n",
        "Y_one_hot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_dqO98GJjxD",
        "colab_type": "text"
      },
      "source": [
        "**Steps to train word2vec model** :\n",
        "1. Initializing weights (introducting parameters that we want to train)\n",
        "2. Propagating forward\n",
        "3. Calculating the cost\n",
        "4. Propagaing backward\n",
        "5. Updating the weights\n",
        "\n",
        "**Initializing weights** : There are 2 layers in the model that needs to be initialised and trained (word embedding layer and dense layer)\n",
        "\n",
        "We want to use a matrix as a shape of the word embeding (vocab_size, emb_size). Because in this way, we can then represent all the vocabularies with **each row representing a word**\n",
        "\n",
        "The shape of the dense layer will be similar manner, but as a **matrix multiplication**. \n",
        "* Its input will be : (emb_size , # of training instances)\n",
        "* Its output to be : (vocab, # of training instances)\n",
        "\n",
        "For each word, we want to calculate what the probability that word appears within the context of the given input word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqv1vF4dMqL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_wrd_emb(vocab_size, emb_size):\n",
        "    WRD_EMB = npy.random.randn(vocab_size, emb_size) * 0.01\n",
        "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
        "    return WRD_EMB\n",
        "\n",
        "def initialize_dense_layer(input_size, output_size):\n",
        "    W = npy.random.randn(output_size, input_size) * 0.01\n",
        "    assert(W.shape == (output_size, input_size))\n",
        "    return W\n",
        "\n",
        "def initialize_parameters(vocab_size, emb_size):\n",
        "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
        "    W = initialize_dense_layer(emb_size, vocab_size)\n",
        "\n",
        "    parameters = {}\n",
        "    parameters['WRD_EMB'] = WRD_EMB\n",
        "    parameters['W'] = W\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIqccVDhMrAQ",
        "colab_type": "text"
      },
      "source": [
        "## Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1lobApxMu3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ind_to_word_vecs(inds, parameters):\n",
        "    m = inds.shape[1]\n",
        "    WRD_EMB = parameters['WRD_EMB']\n",
        "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
        "    assert(word_vec.shape ==(WRD_EMB.shape[1],m))\n",
        "    return word_vec\n",
        "\n",
        "def linear_dense(word_vec, parameters):\n",
        "    m = word_vec.shape[1]\n",
        "    W = parameters['W']\n",
        "    Z = npy.dot(W, word_vec)\n",
        "    assert(Z.shape == (W.shape[0],m))\n",
        "    return W,Z\n",
        "\n",
        "def softmax(Z):\n",
        "    softmax_output = npy.divide(npy.exp(Z), npy.sum(npy.exp(Z),axis=0, keepdims=True) + 0.001)\n",
        "    assert(softmax_output.shape == Z.shape)\n",
        "    return softmax_output\n",
        "\n",
        "def forward_propagation(inds, parameters):\n",
        "    word_vec = ind_to_word_vecs(inds,parameters)\n",
        "    W, Z = linear_dense(word_vec, parameters)\n",
        "    softmax_output = softmax(Z)\n",
        "\n",
        "    caches = {}\n",
        "    caches['inds'] = inds\n",
        "    caches['word_vec'] = word_vec\n",
        "    caches['W'] = W\n",
        "    caches['Z'] = Z\n",
        "\n",
        "    return softmax_output, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJK0uw-6O8Tc",
        "colab_type": "text"
      },
      "source": [
        "#### **Computation of the Cost(L)** using the following formula: \n",
        " ![alt text](https://miro.medium.com/max/233/1*hMXG01hWWU0QAASsUOsPbA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JgvRmBvPQla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy(softmax_out, Y):\n",
        "  m = softmax_out.shape[1]\n",
        "  cost = -(1/m)*npy.sum(npy.sum(Y*npy.log(softmax_out +0.001),\n",
        "                                axis=0, keepdims=True), axis =1)\n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F247YFFPPz4S",
        "colab_type": "text"
      },
      "source": [
        "#### **Back Propagation**\n",
        "\n",
        "This is a process to calculate graidents of the trainable weights with respect to the loss function and update the weight with its associated gradient.\n",
        "This process uses **Chain Rule** in Calculus:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/106/1*XNf0iIOFu0SKOWnWQKquXg.png)\n",
        "\n",
        "We need to apply this function to all the dense layers and word embedding layers in the matrix that we would like to train. Hence:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/217/1*YReZZYTlRC07ZVDlMi90Jw.png)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/686/1*6B8d4lwqaPRsmWHNDl0Efw.png)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/806/1*P7sM_vmmkpdtwgk90mi7wQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNxSOzS8RYAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_back_propagation(Y, softmax_out):\n",
        "  dL_dZ = softmax_out-Y\n",
        "  assert(dL_dZ.shape == softmax_out.shape)\n",
        "  return dL_dZ\n",
        "\n",
        "def dense_layer_back_propagation(dL_dZ, caches):\n",
        "  W = caches['W']\n",
        "  word_vec = caches['word_vec']\n",
        "  m = word_vec.shape[1]\n",
        "\n",
        "  dL_dW = (1/m) * npy.dot(dL_dZ, word_vec.T)\n",
        "  dL_dword_vec = npy.dot(W.T, dL_dZ)\n",
        "\n",
        "  assert(W.shape == dL_dW.shape)\n",
        "  assert(word_vec.shape == dL_dword_vec.shape)\n",
        "  return dL_dW, dL_dword_vec\n",
        "\n",
        "def backward_propagation(Y, softmax_out, caches):\n",
        "  dL_dZ = softmax_back_propagation(Y, softmax_out)\n",
        "  dL_dW, dL_dword_vec = dense_layer_back_propagation(dL_dZ, caches)\n",
        "\n",
        "  gradients = dict()\n",
        "  gradients['dL_dZ'] = dL_dZ\n",
        "  gradients['dL_dW'] = dL_dW\n",
        "  gradients['dL_dword_vec'] = dL_dword_vec\n",
        "\n",
        "  return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-wXhG4eTcAt",
        "colab_type": "text"
      },
      "source": [
        "#### Updating parameters base on the backwawrd propagation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhumwosRThyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, caches, gradients, learning_rate):\n",
        "  vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
        "  inds = caches['inds']\n",
        "  WRD_EMB = parameters['WRD_EMB']\n",
        "  dL_dword_vec = gradients['dL_dword_vec']\n",
        "  m = inds.shape[-1]\n",
        "\n",
        "  parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
        "  parameters['W'] -= learning_rate * gradients['dL_dW']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4qem83wUbOh",
        "colab_type": "text"
      },
      "source": [
        "### Model Training\n",
        "\n",
        "Repeating the process of forward propagation, backward propagation and weight updating to train the model.\n",
        "*Note: The cost after each epoch should show a decreasing trend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIPce0xXU99w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256,\n",
        "                            parameters=None, print_cost=False, plot_cost=True):\n",
        "  costs = []\n",
        "  m = X.shape[1]\n",
        "\n",
        "  if parameters is None:\n",
        "    parameters = initialize_parameters(vocab_size, emb_size)\n",
        "  \n",
        "  begin_time = datetime.now()\n",
        "  for epoch in range(epochs):\n",
        "      epoch_cost = 0\n",
        "      batch_inds = list(range(0, m, batch_size))\n",
        "      npy.random.shuffle(batch_inds)\n",
        "      for i in batch_inds:\n",
        "          X_batch = X[:, i:i+batch_size]\n",
        "          Y_batch = Y[:, i:i+batch_size]\n",
        "\n",
        "          softmax_out, caches = forward_propagation(X_batch, parameters)\n",
        "          gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
        "          update_parameters(parameters, caches, gradients, learning_rate)\n",
        "          cost = cross_entropy(softmax_out, Y_batch)\n",
        "          epoch_cost += npy.squeeze(cost)\n",
        "      costs.append(epoch_cost)\n",
        "      if print_cost and epoch % (epochs // 500) == 0:\n",
        "        print(\"Cost after epoch {} : {} \".format(epoch, epoch_cost))\n",
        "      if epoch % (epochs // 100) == 0:\n",
        "        learning_rate *=0.98\n",
        "  end_time = datetime.now()\n",
        "  print('training time: {}'.format(end_time - begin_time))\n",
        "\n",
        "  if plot_cost:\n",
        "      plt.plot(npy.arange(epochs), costs)\n",
        "      plt.xlabel(' Number of epochs')\n",
        "      plt.ylabel('Cost')\n",
        "  return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8c07x0zVHN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35fbf6da-ff5f-4375-b1ad-1d1aee1fae72"
      },
      "source": [
        "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128,\n",
        "                                parameters=None, print_cost=True)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after epoch 0 : 2.5520900251319643 \n",
            "Cost after epoch 10 : 2.5518143695645428 \n",
            "Cost after epoch 20 : 2.551527811153713 \n",
            "Cost after epoch 30 : 2.551210776228229 \n",
            "Cost after epoch 40 : 2.5508435215286656 \n",
            "Cost after epoch 50 : 2.5504050105466622 \n",
            "Cost after epoch 60 : 2.5498828599114804 \n",
            "Cost after epoch 70 : 2.5492472926201315 \n",
            "Cost after epoch 80 : 2.548470578650179 \n",
            "Cost after epoch 90 : 2.5475224303764175 \n",
            "Cost after epoch 100 : 2.5463683700049486 \n",
            "Cost after epoch 110 : 2.5449965359790974 \n",
            "Cost after epoch 120 : 2.5433492581637585 \n",
            "Cost after epoch 130 : 2.5413758803690683 \n",
            "Cost after epoch 140 : 2.539021888758969 \n",
            "Cost after epoch 150 : 2.536225955724305 \n",
            "Cost after epoch 160 : 2.5329840121192375 \n",
            "Cost after epoch 170 : 2.529187310004377 \n",
            "Cost after epoch 180 : 2.524753777273213 \n",
            "Cost after epoch 190 : 2.5196035867273 \n",
            "Cost after epoch 200 : 2.513655806704972 \n",
            "Cost after epoch 210 : 2.5069631747602497 \n",
            "Cost after epoch 220 : 2.499377187743015 \n",
            "Cost after epoch 230 : 2.490835501832019 \n",
            "Cost after epoch 240 : 2.4813127373318524 \n",
            "Cost after epoch 250 : 2.470818014753912 \n",
            "Cost after epoch 260 : 2.459618317802958 \n",
            "Cost after epoch 270 : 2.4476613387374795 \n",
            "Cost after epoch 280 : 2.435083056174784 \n",
            "Cost after epoch 290 : 2.4220965359827673 \n",
            "Cost after epoch 300 : 2.4089611306210745 \n",
            "Cost after epoch 310 : 2.3961949860205607 \n",
            "Cost after epoch 320 : 2.383858630494842 \n",
            "Cost after epoch 330 : 2.3721661617366037 \n",
            "Cost after epoch 340 : 2.361299326081808 \n",
            "Cost after epoch 350 : 2.3513603021749194 \n",
            "Cost after epoch 360 : 2.3425177381604265 \n",
            "Cost after epoch 370 : 2.334535978164236 \n",
            "Cost after epoch 380 : 2.327272561670544 \n",
            "Cost after epoch 390 : 2.320563973117547 \n",
            "Cost after epoch 400 : 2.3142271097712483 \n",
            "Cost after epoch 410 : 2.3081868497866282 \n",
            "Cost after epoch 420 : 2.302173262730189 \n",
            "Cost after epoch 430 : 2.29602823635735 \n",
            "Cost after epoch 440 : 2.2896304682239035 \n",
            "Cost after epoch 450 : 2.282886814778725 \n",
            "Cost after epoch 460 : 2.2758670702031756 \n",
            "Cost after epoch 470 : 2.2684382306969404 \n",
            "Cost after epoch 480 : 2.2605838956524402 \n",
            "Cost after epoch 490 : 2.252327157377531 \n",
            "Cost after epoch 500 : 2.2437134590889274 \n",
            "Cost after epoch 510 : 2.2349691449622435 \n",
            "Cost after epoch 520 : 2.2260332608878053 \n",
            "Cost after epoch 530 : 2.2169684435597974 \n",
            "Cost after epoch 540 : 2.2078581367577814 \n",
            "Cost after epoch 550 : 2.1987848308742044 \n",
            "Cost after epoch 560 : 2.189986706125876 \n",
            "Cost after epoch 570 : 2.18138367812834 \n",
            "Cost after epoch 580 : 2.1730153985730314 \n",
            "Cost after epoch 590 : 2.164929872936096 \n",
            "Cost after epoch 600 : 2.1571649270773343 \n",
            "Cost after epoch 610 : 2.1498791120677234 \n",
            "Cost after epoch 620 : 2.1429602933386263 \n",
            "Cost after epoch 630 : 2.1364022977804042 \n",
            "Cost after epoch 640 : 2.1302070158038497 \n",
            "Cost after epoch 650 : 2.124370199098489 \n",
            "Cost after epoch 660 : 2.1189787796305395 \n",
            "Cost after epoch 670 : 2.1139216767066684 \n",
            "Cost after epoch 680 : 2.1091725569279918 \n",
            "Cost after epoch 690 : 2.104714769400832 \n",
            "Cost after epoch 700 : 2.1005307700575377 \n",
            "Cost after epoch 710 : 2.0966716788061377 \n",
            "Cost after epoch 720 : 2.0930500814983284 \n",
            "Cost after epoch 730 : 2.08964199022278 \n",
            "Cost after epoch 740 : 2.086432422793306 \n",
            "Cost after epoch 750 : 2.0834074059075824 \n",
            "Cost after epoch 760 : 2.0806042466840626 \n",
            "Cost after epoch 770 : 2.0779607124866173 \n",
            "Cost after epoch 780 : 2.0754607853723877 \n",
            "Cost after epoch 790 : 2.0730952973980914 \n",
            "Cost after epoch 800 : 2.0708559595759826 \n",
            "Cost after epoch 810 : 2.0687726909727306 \n",
            "Cost after epoch 820 : 2.066801389733753 \n",
            "Cost after epoch 830 : 2.0649319042644563 \n",
            "Cost after epoch 840 : 2.063159000931808 \n",
            "Cost after epoch 850 : 2.06147791766966 \n",
            "Cost after epoch 860 : 2.059912455369802 \n",
            "Cost after epoch 870 : 2.0584305411133514 \n",
            "Cost after epoch 880 : 2.0570254193057056 \n",
            "Cost after epoch 890 : 2.0556938934397544 \n",
            "Cost after epoch 900 : 2.054432999733169 \n",
            "Cost after epoch 910 : 2.053261065952944 \n",
            "Cost after epoch 920 : 2.0521543719818784 \n",
            "Cost after epoch 930 : 2.0511081717631283 \n",
            "Cost after epoch 940 : 2.050120334696459 \n",
            "Cost after epoch 950 : 2.049188858267871 \n",
            "Cost after epoch 960 : 2.048327353204223 \n",
            "Cost after epoch 970 : 2.0475183166425768 \n",
            "Cost after epoch 980 : 2.0467583250436583 \n",
            "Cost after epoch 990 : 2.046045862160475 \n",
            "Cost after epoch 1000 : 2.045379481771579 \n",
            "Cost after epoch 1010 : 2.0447687733833964 \n",
            "Cost after epoch 1020 : 2.044201043505203 \n",
            "Cost after epoch 1030 : 2.0436737621869554 \n",
            "Cost after epoch 1040 : 2.0431857385440666 \n",
            "Cost after epoch 1050 : 2.042735806094257 \n",
            "Cost after epoch 1060 : 2.042330092248145 \n",
            "Cost after epoch 1070 : 2.0419596651409693 \n",
            "Cost after epoch 1080 : 2.0416225677250655 \n",
            "Cost after epoch 1090 : 2.0413177232255793 \n",
            "Cost after epoch 1100 : 2.0410440497451097 \n",
            "Cost after epoch 1110 : 2.040804736144876 \n",
            "Cost after epoch 1120 : 2.04059380718515 \n",
            "Cost after epoch 1130 : 2.0404096713524424 \n",
            "Cost after epoch 1140 : 2.040251258894708 \n",
            "Cost after epoch 1150 : 2.040117486797801 \n",
            "Cost after epoch 1160 : 2.0400091870789616 \n",
            "Cost after epoch 1170 : 2.0399227138121545 \n",
            "Cost after epoch 1180 : 2.0398567466642894 \n",
            "Cost after epoch 1190 : 2.039810218394046 \n",
            "Cost after epoch 1200 : 2.0397820564094795 \n",
            "Cost after epoch 1210 : 2.039771371349219 \n",
            "Cost after epoch 1220 : 2.0397763305098575 \n",
            "Cost after epoch 1230 : 2.0397958623637926 \n",
            "Cost after epoch 1240 : 2.0398289560246394 \n",
            "Cost after epoch 1250 : 2.039874609284959 \n",
            "Cost after epoch 1260 : 2.039930833775219 \n",
            "Cost after epoch 1270 : 2.0399971574921403 \n",
            "Cost after epoch 1280 : 2.0400727588621894 \n",
            "Cost after epoch 1290 : 2.04015674743794 \n",
            "Cost after epoch 1300 : 2.0402482542007707 \n",
            "Cost after epoch 1310 : 2.040344733749495 \n",
            "Cost after epoch 1320 : 2.040446670262023 \n",
            "Cost after epoch 1330 : 2.040553487596242 \n",
            "Cost after epoch 1340 : 2.0406644623839485 \n",
            "Cost after epoch 1350 : 2.0407789010352064 \n",
            "Cost after epoch 1360 : 2.0408941242656593 \n",
            "Cost after epoch 1370 : 2.0410112220982444 \n",
            "Cost after epoch 1380 : 2.0411298449494404 \n",
            "Cost after epoch 1390 : 2.0412494577594464 \n",
            "Cost after epoch 1400 : 2.04136955876416 \n",
            "Cost after epoch 1410 : 2.041487628608073 \n",
            "Cost after epoch 1420 : 2.041605077756255 \n",
            "Cost after epoch 1430 : 2.0417217515736787 \n",
            "Cost after epoch 1440 : 2.041837300813039 \n",
            "Cost after epoch 1450 : 2.041951408691907 \n",
            "Cost after epoch 1460 : 2.042061887387448 \n",
            "Cost after epoch 1470 : 2.042170252073892 \n",
            "Cost after epoch 1480 : 2.042276502451359 \n",
            "Cost after epoch 1490 : 2.042380453042589 \n",
            "Cost after epoch 1500 : 2.0424819463417223 \n",
            "Cost after epoch 1510 : 2.0425791929599444 \n",
            "Cost after epoch 1520 : 2.0426736632914566 \n",
            "Cost after epoch 1530 : 2.0427654651756044 \n",
            "Cost after epoch 1540 : 2.042854540236857 \n",
            "Cost after epoch 1550 : 2.042940850902359 \n",
            "Cost after epoch 1560 : 2.043022993408512 \n",
            "Cost after epoch 1570 : 2.0431023010531955 \n",
            "Cost after epoch 1580 : 2.043178942100992 \n",
            "Cost after epoch 1590 : 2.0432529405590616 \n",
            "Cost after epoch 1600 : 2.0433243328891892 \n",
            "Cost after epoch 1610 : 2.0433920388202367 \n",
            "Cost after epoch 1620 : 2.0434572079802056 \n",
            "Cost after epoch 1630 : 2.0435200254259835 \n",
            "Cost after epoch 1640 : 2.043580553246364 \n",
            "Cost after epoch 1650 : 2.043638858274995 \n",
            "Cost after epoch 1660 : 2.043694103725354 \n",
            "Cost after epoch 1670 : 2.0437472429585815 \n",
            "Cost after epoch 1680 : 2.0437984443485444 \n",
            "Cost after epoch 1690 : 2.0438477726510005 \n",
            "Cost after epoch 1700 : 2.043895291843025 \n",
            "Cost after epoch 1710 : 2.043940336814197 \n",
            "Cost after epoch 1720 : 2.043983674611294 \n",
            "Cost after epoch 1730 : 2.044025437644561 \n",
            "Cost after epoch 1740 : 2.044065672237985 \n",
            "Cost after epoch 1750 : 2.0441044212904176 \n",
            "Cost after epoch 1760 : 2.04414114291219 \n",
            "Cost after epoch 1770 : 2.0441764396353204 \n",
            "Cost after epoch 1780 : 2.0442104028689094 \n",
            "Cost after epoch 1790 : 2.044243054205818 \n",
            "Cost after epoch 1800 : 2.0442744117321756 \n",
            "Cost after epoch 1810 : 2.044304034453422 \n",
            "Cost after epoch 1820 : 2.044332385035415 \n",
            "Cost after epoch 1830 : 2.044359519375058 \n",
            "Cost after epoch 1840 : 2.044385439013732 \n",
            "Cost after epoch 1850 : 2.044410143515725 \n",
            "Cost after epoch 1860 : 2.0444332901807947 \n",
            "Cost after epoch 1870 : 2.044455223335039 \n",
            "Cost after epoch 1880 : 2.0444759740156777 \n",
            "Cost after epoch 1890 : 2.0444955337610424 \n",
            "Cost after epoch 1900 : 2.044513894223249 \n",
            "Cost after epoch 1910 : 2.0445308179600374 \n",
            "Cost after epoch 1920 : 2.0445465470462483 \n",
            "Cost after epoch 1930 : 2.044561098786221 \n",
            "Cost after epoch 1940 : 2.044574465209489 \n",
            "Cost after epoch 1950 : 2.044586640329442 \n",
            "Cost after epoch 1960 : 2.044597499126392 \n",
            "Cost after epoch 1970 : 2.0446071927409406 \n",
            "Cost after epoch 1980 : 2.044615733469233 \n",
            "Cost after epoch 1990 : 2.0446231216805977 \n",
            "Cost after epoch 2000 : 2.0446293609456685 \n",
            "Cost after epoch 2010 : 2.0446344397121723 \n",
            "Cost after epoch 2020 : 2.044638426853522 \n",
            "Cost after epoch 2030 : 2.04464133481983 \n",
            "Cost after epoch 2040 : 2.0446431761879587 \n",
            "Cost after epoch 2050 : 2.0446439671502796 \n",
            "Cost after epoch 2060 : 2.0446438016791 \n",
            "Cost after epoch 2070 : 2.0446426745451887 \n",
            "Cost after epoch 2080 : 2.044640599946833 \n",
            "Cost after epoch 2090 : 2.0446376024884234 \n",
            "Cost after epoch 2100 : 2.044633710053291 \n",
            "Cost after epoch 2110 : 2.0446291053831303 \n",
            "Cost after epoch 2120 : 2.044623718213885 \n",
            "Cost after epoch 2130 : 2.044617563221836 \n",
            "Cost after epoch 2140 : 2.044610673611238 \n",
            "Cost after epoch 2150 : 2.044603084976697 \n",
            "Cost after epoch 2160 : 2.0445950457578856 \n",
            "Cost after epoch 2170 : 2.04458643111487 \n",
            "Cost after epoch 2180 : 2.0445772531828488 \n",
            "Cost after epoch 2190 : 2.0445675484254995 \n",
            "Cost after epoch 2200 : 2.0445573545348337 \n",
            "Cost after epoch 2210 : 2.044546960101989 \n",
            "Cost after epoch 2220 : 2.0445361972356952 \n",
            "Cost after epoch 2230 : 2.0445250719832027 \n",
            "Cost after epoch 2240 : 2.0445136182959276 \n",
            "Cost after epoch 2250 : 2.0445018701947206 \n",
            "Cost after epoch 2260 : 2.0444901324709654 \n",
            "Cost after epoch 2270 : 2.0444782058422053 \n",
            "Cost after epoch 2280 : 2.044466087201593 \n",
            "Cost after epoch 2290 : 2.044453803025015 \n",
            "Cost after epoch 2300 : 2.044441378908843 \n",
            "Cost after epoch 2310 : 2.04442911683226 \n",
            "Cost after epoch 2320 : 2.044416796667759 \n",
            "Cost after epoch 2330 : 2.04440440416008 \n",
            "Cost after epoch 2340 : 2.044391954913211 \n",
            "Cost after epoch 2350 : 2.0443794630215324 \n",
            "Cost after epoch 2360 : 2.044367215654 \n",
            "Cost after epoch 2370 : 2.0443549800310663 \n",
            "Cost after epoch 2380 : 2.0443427299803925 \n",
            "Cost after epoch 2390 : 2.044330468653514 \n",
            "Cost after epoch 2400 : 2.0443181974016973 \n",
            "Cost after epoch 2410 : 2.044306184317992 \n",
            "Cost after epoch 2420 : 2.0442941888744897 \n",
            "Cost after epoch 2430 : 2.044282173274196 \n",
            "Cost after epoch 2440 : 2.044270128312506 \n",
            "Cost after epoch 2450 : 2.044258042994146 \n",
            "Cost after epoch 2460 : 2.0442461693827263 \n",
            "Cost after epoch 2470 : 2.044234260358227 \n",
            "Cost after epoch 2480 : 2.0442222675307913 \n",
            "Cost after epoch 2490 : 2.044210170754392 \n",
            "Cost after epoch 2500 : 2.0441979483330637 \n",
            "Cost after epoch 2510 : 2.044185845449483 \n",
            "Cost after epoch 2520 : 2.0441736058551245 \n",
            "Cost after epoch 2530 : 2.044161171990649 \n",
            "Cost after epoch 2540 : 2.044148515066371 \n",
            "Cost after epoch 2550 : 2.044135605137312 \n",
            "Cost after epoch 2560 : 2.044122694104396 \n",
            "Cost after epoch 2570 : 2.0441095108533625 \n",
            "Cost after epoch 2580 : 2.044095990173399 \n",
            "Cost after epoch 2590 : 2.044082097391787 \n",
            "Cost after epoch 2600 : 2.04406779715671 \n",
            "Cost after epoch 2610 : 2.044053364333205 \n",
            "Cost after epoch 2620 : 2.0440385047977014 \n",
            "Cost after epoch 2630 : 2.044023147084755 \n",
            "Cost after epoch 2640 : 2.0440072535045983 \n",
            "Cost after epoch 2650 : 2.0439907861849482 \n",
            "Cost after epoch 2660 : 2.0439740601054375 \n",
            "Cost after epoch 2670 : 2.0439567485332906 \n",
            "Cost after epoch 2680 : 2.0439387748914095 \n",
            "Cost after epoch 2690 : 2.0439201011678887 \n",
            "Cost after epoch 2700 : 2.0439006896377565 \n",
            "Cost after epoch 2710 : 2.043880911916308 \n",
            "Cost after epoch 2720 : 2.043860398580263 \n",
            "Cost after epoch 2730 : 2.0438390687690933 \n",
            "Cost after epoch 2740 : 2.0438168864795956 \n",
            "Cost after epoch 2750 : 2.0437938164042566 \n",
            "Cost after epoch 2760 : 2.0437703014483706 \n",
            "Cost after epoch 2770 : 2.0437459198341674 \n",
            "Cost after epoch 2780 : 2.0437205869265815 \n",
            "Cost after epoch 2790 : 2.043694270582485 \n",
            "Cost after epoch 2800 : 2.0436669396820624 \n",
            "Cost after epoch 2810 : 2.043639120309032 \n",
            "Cost after epoch 2820 : 2.0436103298385504 \n",
            "Cost after epoch 2830 : 2.043580480086566 \n",
            "Cost after epoch 2840 : 2.0435495440939153 \n",
            "Cost after epoch 2850 : 2.0435174961623264 \n",
            "Cost after epoch 2860 : 2.0434849541189255 \n",
            "Cost after epoch 2870 : 2.0434513677415653 \n",
            "Cost after epoch 2880 : 2.0434166453013978 \n",
            "Cost after epoch 2890 : 2.0433807658262655 \n",
            "Cost after epoch 2900 : 2.0433437097529437 \n",
            "Cost after epoch 2910 : 2.0433061916520048 \n",
            "Cost after epoch 2920 : 2.043267589070709 \n",
            "Cost after epoch 2930 : 2.0432278065932685 \n",
            "Cost after epoch 2940 : 2.043186829563972 \n",
            "Cost after epoch 2950 : 2.043144644802085 \n",
            "Cost after epoch 2960 : 2.0431020650854945 \n",
            "Cost after epoch 2970 : 2.043058393399119 \n",
            "Cost after epoch 2980 : 2.043013530428942 \n",
            "Cost after epoch 2990 : 2.0429674677652887 \n",
            "Cost after epoch 3000 : 2.0429201984696306 \n",
            "Cost after epoch 3010 : 2.042872631696675 \n",
            "Cost after epoch 3020 : 2.0428239958540133 \n",
            "Cost after epoch 3030 : 2.0427741875196843 \n",
            "Cost after epoch 3040 : 2.0427232041470553 \n",
            "Cost after epoch 3050 : 2.0426710446014273 \n",
            "Cost after epoch 3060 : 2.0426187096832757 \n",
            "Cost after epoch 3070 : 2.04256535540006 \n",
            "Cost after epoch 3080 : 2.0425108740720614 \n",
            "Cost after epoch 3090 : 2.042455268409958 \n",
            "Cost after epoch 3100 : 2.0423985424367497 \n",
            "Cost after epoch 3110 : 2.0423417814545344 \n",
            "Cost after epoch 3120 : 2.042284073209573 \n",
            "Cost after epoch 3130 : 2.042225305717069 \n",
            "Cost after epoch 3140 : 2.0421654862007004 \n",
            "Cost after epoch 3150 : 2.042104623070549 \n",
            "Cost after epoch 3160 : 2.042043877102879 \n",
            "Cost after epoch 3170 : 2.0419822735816187 \n",
            "Cost after epoch 3180 : 2.041919696297939 \n",
            "Cost after epoch 3190 : 2.0418561561786586 \n",
            "Cost after epoch 3200 : 2.041791665197039 \n",
            "Cost after epoch 3210 : 2.04172744922318 \n",
            "Cost after epoch 3220 : 2.0416624781963164 \n",
            "Cost after epoch 3230 : 2.0415966319031704 \n",
            "Cost after epoch 3240 : 2.0415299241572886 \n",
            "Cost after epoch 3250 : 2.0414623696746634 \n",
            "Cost after epoch 3260 : 2.0413952481859376 \n",
            "Cost after epoch 3270 : 2.041327482369182 \n",
            "Cost after epoch 3280 : 2.041258948361536 \n",
            "Cost after epoch 3290 : 2.0411896620845655 \n",
            "Cost after epoch 3300 : 2.041119640221845 \n",
            "Cost after epoch 3310 : 2.0410502047267154 \n",
            "Cost after epoch 3320 : 2.040980239722975 \n",
            "Cost after epoch 3330 : 2.0409096181746924 \n",
            "Cost after epoch 3340 : 2.0408383574007667 \n",
            "Cost after epoch 3350 : 2.040766475350559 \n",
            "Cost after epoch 3360 : 2.0406953245424533 \n",
            "Cost after epoch 3370 : 2.0406237595603605 \n",
            "Cost after epoch 3380 : 2.040551650772685 \n",
            "Cost after epoch 3390 : 2.0404790162711417 \n",
            "Cost after epoch 3400 : 2.0404058746586187 \n",
            "Cost after epoch 3410 : 2.040333597657335 \n",
            "Cost after epoch 3420 : 2.0402610193546806 \n",
            "Cost after epoch 3430 : 2.040188008176901 \n",
            "Cost after epoch 3440 : 2.0401145824584503 \n",
            "Cost after epoch 3450 : 2.0400407609396627 \n",
            "Cost after epoch 3460 : 2.0399679237301593 \n",
            "Cost after epoch 3470 : 2.0398948932579097 \n",
            "Cost after epoch 3480 : 2.0398215367104124 \n",
            "Cost after epoch 3490 : 2.0397478722299494 \n",
            "Cost after epoch 3500 : 2.0396739182739005 \n",
            "Cost after epoch 3510 : 2.039601053252397 \n",
            "Cost after epoch 3520 : 2.0395280963747564 \n",
            "Cost after epoch 3530 : 2.0394549143175214 \n",
            "Cost after epoch 3540 : 2.03938152468647 \n",
            "Cost after epoch 3550 : 2.039307945325821 \n",
            "Cost after epoch 3560 : 2.039235543716787 \n",
            "Cost after epoch 3570 : 2.0391631437484707 \n",
            "Cost after epoch 3580 : 2.039090612315285 \n",
            "Cost after epoch 3590 : 2.0390179662247276 \n",
            "Cost after epoch 3600 : 2.0389452224592346 \n",
            "Cost after epoch 3610 : 2.0388737292780963 \n",
            "Cost after epoch 3620 : 2.0388023225012053 \n",
            "Cost after epoch 3630 : 2.038730869950456 \n",
            "Cost after epoch 3640 : 2.0386593874459282 \n",
            "Cost after epoch 3650 : 2.038587890930988 \n",
            "Cost after epoch 3660 : 2.0385177021246537 \n",
            "Cost after epoch 3670 : 2.038447675312448 \n",
            "Cost after epoch 3680 : 2.0383776799174815 \n",
            "Cost after epoch 3690 : 2.0383077306452906 \n",
            "Cost after epoch 3700 : 2.0382378422834138 \n",
            "Cost after epoch 3710 : 2.038169303672343 \n",
            "Cost after epoch 3720 : 2.038100993344412 \n",
            "Cost after epoch 3730 : 2.0380327829488496 \n",
            "Cost after epoch 3740 : 2.0379646860009917 \n",
            "Cost after epoch 3750 : 2.0378967160657626 \n",
            "Cost after epoch 3760 : 2.0378301237269625 \n",
            "Cost after epoch 3770 : 2.0377638167796297 \n",
            "Cost after epoch 3780 : 2.0376976696657803 \n",
            "Cost after epoch 3790 : 2.0376316946764916 \n",
            "Cost after epoch 3800 : 2.03756590412745 \n",
            "Cost after epoch 3810 : 2.037501505888635 \n",
            "Cost after epoch 3820 : 2.0374374412775467 \n",
            "Cost after epoch 3830 : 2.037373588031264 \n",
            "Cost after epoch 3840 : 2.0373099572152458 \n",
            "Cost after epoch 3850 : 2.037246559900695 \n",
            "Cost after epoch 3860 : 2.0371845576551983 \n",
            "Cost after epoch 3870 : 2.037122928847439 \n",
            "Cost after epoch 3880 : 2.037061554944127 \n",
            "Cost after epoch 3890 : 2.0370004458086908 \n",
            "Cost after epoch 3900 : 2.0369396112963907 \n",
            "Cost after epoch 3910 : 2.036880163890972 \n",
            "Cost after epoch 3920 : 2.0368211218469456 \n",
            "Cost after epoch 3930 : 2.0367623707265663 \n",
            "Cost after epoch 3940 : 2.036703919233202 \n",
            "Cost after epoch 3950 : 2.0366457760520844 \n",
            "Cost after epoch 3960 : 2.036589002542179 \n",
            "Cost after epoch 3970 : 2.0365326590295747 \n",
            "Cost after epoch 3980 : 2.036476635470304 \n",
            "Cost after epoch 3990 : 2.0364209394625963 \n",
            "Cost after epoch 4000 : 2.0363655785796837 \n",
            "Cost after epoch 4010 : 2.036311561686416 \n",
            "Cost after epoch 4020 : 2.036257992767722 \n",
            "Cost after epoch 4030 : 2.0362047664064495 \n",
            "Cost after epoch 4040 : 2.036151889159197 \n",
            "Cost after epoch 4050 : 2.0360993675530956 \n",
            "Cost after epoch 4060 : 2.0360481571921825 \n",
            "Cost after epoch 4070 : 2.035997406762231 \n",
            "Cost after epoch 4080 : 2.0359470156443495 \n",
            "Cost after epoch 4090 : 2.0358969894221333 \n",
            "Cost after epoch 4100 : 2.035847333647093 \n",
            "Cost after epoch 4110 : 2.035798950430964 \n",
            "Cost after epoch 4120 : 2.0357510337112816 \n",
            "Cost after epoch 4130 : 2.035703487781625 \n",
            "Cost after epoch 4140 : 2.0356563173237165 \n",
            "Cost after epoch 4150 : 2.0356095269859686 \n",
            "Cost after epoch 4160 : 2.0355639656268183 \n",
            "Cost after epoch 4170 : 2.035518872549035 \n",
            "Cost after epoch 4180 : 2.0354741570205275 \n",
            "Cost after epoch 4190 : 2.035429822892791 \n",
            "Cost after epoch 4200 : 2.0353858739838384 \n",
            "Cost after epoch 4210 : 2.0353431065470544 \n",
            "Cost after epoch 4220 : 2.035300804978529 \n",
            "Cost after epoch 4230 : 2.0352588835351457 \n",
            "Cost after epoch 4240 : 2.0352173453089075 \n",
            "Cost after epoch 4250 : 2.035176193358952 \n",
            "Cost after epoch 4260 : 2.0351361723337904 \n",
            "Cost after epoch 4270 : 2.0350966111152498 \n",
            "Cost after epoch 4280 : 2.03505742892184 \n",
            "Cost after epoch 4290 : 2.0350186281549103 \n",
            "Cost after epoch 4300 : 2.034980211184148 \n",
            "Cost after epoch 4310 : 2.0349428723521177 \n",
            "Cost after epoch 4320 : 2.034905984130036 \n",
            "Cost after epoch 4330 : 2.034869470633862 \n",
            "Cost after epoch 4340 : 2.034833333640568 \n",
            "Cost after epoch 4350 : 2.0347975748971185 \n",
            "Cost after epoch 4360 : 2.0347628399888276 \n",
            "Cost after epoch 4370 : 2.034728543834982 \n",
            "Cost after epoch 4380 : 2.034694615352976 \n",
            "Cost after epoch 4390 : 2.034661055758716 \n",
            "Cost after epoch 4400 : 2.034627866240088 \n",
            "Cost after epoch 4410 : 2.034595645378816 \n",
            "Cost after epoch 4420 : 2.0345638491964793 \n",
            "Cost after epoch 4430 : 2.0345324112888132 \n",
            "Cost after epoch 4440 : 2.034501332370769 \n",
            "Cost after epoch 4450 : 2.0344706131315236 \n",
            "Cost after epoch 4460 : 2.034440807067005 \n",
            "Cost after epoch 4470 : 2.0344114097881008 \n",
            "Cost after epoch 4480 : 2.0343823594231525 \n",
            "Cost after epoch 4490 : 2.0343536562429727 \n",
            "Cost after epoch 4500 : 2.0343253004950266 \n",
            "Cost after epoch 4510 : 2.0342978026343883 \n",
            "Cost after epoch 4520 : 2.0342706962150485 \n",
            "Cost after epoch 4530 : 2.034243923733725 \n",
            "Cost after epoch 4540 : 2.0342174850705668 \n",
            "Cost after epoch 4550 : 2.034191380084951 \n",
            "Cost after epoch 4560 : 2.034166078329797 \n",
            "Cost after epoch 4570 : 2.0341411495536392 \n",
            "Cost after epoch 4580 : 2.0341165404426187 \n",
            "Cost after epoch 4590 : 2.0340922505363896 \n",
            "Cost after epoch 4600 : 2.034068279356511 \n",
            "Cost after epoch 4610 : 2.0340450577562184 \n",
            "Cost after epoch 4620 : 2.0340221898555355 \n",
            "Cost after epoch 4630 : 2.03399962633978 \n",
            "Cost after epoch 4640 : 2.033977366455019 \n",
            "Cost after epoch 4650 : 2.0339554094319605 \n",
            "Cost after epoch 4660 : 2.033934149663609 \n",
            "Cost after epoch 4670 : 2.0339132237689115 \n",
            "Cost after epoch 4680 : 2.0338925862339328 \n",
            "Cost after epoch 4690 : 2.033872236054849 \n",
            "Cost after epoch 4700 : 2.0338521722152465 \n",
            "Cost after epoch 4710 : 2.033832754900502 \n",
            "Cost after epoch 4720 : 2.0338136513287326 \n",
            "Cost after epoch 4730 : 2.0337948195828877 \n",
            "Cost after epoch 4740 : 2.0337762584498025 \n",
            "Cost after epoch 4750 : 2.0337579667064887 \n",
            "Cost after epoch 4760 : 2.033740272575446 \n",
            "Cost after epoch 4770 : 2.0337228719667726 \n",
            "Cost after epoch 4780 : 2.0337057263534666 \n",
            "Cost after epoch 4790 : 2.0336888343504866 \n",
            "Cost after epoch 4800 : 2.033672194565695 \n",
            "Cost after epoch 4810 : 2.0336561054772004 \n",
            "Cost after epoch 4820 : 2.03364028978986 \n",
            "Cost after epoch 4830 : 2.03362471215906 \n",
            "Cost after epoch 4840 : 2.033609371062321 \n",
            "Cost after epoch 4850 : 2.033594264972726 \n",
            "Cost after epoch 4860 : 2.033579664800267 \n",
            "Cost after epoch 4870 : 2.0335653181724553 \n",
            "Cost after epoch 4880 : 2.033551192720478 \n",
            "Cost after epoch 4890 : 2.033537286815883 \n",
            "Cost after epoch 4900 : 2.0335235988283467 \n",
            "Cost after epoch 4910 : 2.033510374220158 \n",
            "Cost after epoch 4920 : 2.033497383707584 \n",
            "Cost after epoch 4930 : 2.0334845976937115 \n",
            "Cost after epoch 4940 : 2.033472014472683 \n",
            "Cost after epoch 4950 : 2.0334596323392127 \n",
            "Cost after epoch 4960 : 2.0334476733609375 \n",
            "Cost after epoch 4970 : 2.033435929558311 \n",
            "Cost after epoch 4980 : 2.033424373906517 \n",
            "Cost after epoch 4990 : 2.033413004648029 \n",
            "training time: 0:00:01.196956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCNFcWGJbPnb",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating results\n",
        "\n",
        "The input sentence was :\n",
        "\n",
        "After the deduction of the cost of investing beating the stock market is a loser's game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd8G4BWzbUOr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2db15eba-52e2-43f7-f79e-14357e63f2ea"
      },
      "source": [
        "X_test = npy.arange(vocab_size)\n",
        "X_test = npy.expand_dims(X_test, axis=0)\n",
        "softmax_test, _ = forward_propagation(X_test, paras)\n",
        "top_sorted_inds = npy.argsort(softmax_test, axis=0)[-4:,:]\n",
        "top_sorted_inds"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6,  6,  9,  4,  1,  6,  2,  2,  4,  4,  6,  8,  2],\n",
              "       [ 8,  8, 10,  3, 11,  2,  7,  6,  3,  6,  9,  3, 10],\n",
              "       [ 3,  3,  5,  1,  8,  5,  4,  4,  1,  5, 12,  1,  9],\n",
              "       [11, 11,  6,  0,  7,  9,  1,  9,  0, 11,  2,  0,  5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4e9QdLzcDId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "bc44c18a-5986-49cf-e4ee-994e906b849e"
      },
      "source": [
        "for input_ind in range(vocab_size):\n",
        "    input_word = id_to_word[input_ind]\n",
        "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
        "    print(\"{}'s neighbour words: {}\".format(input_word, output_words))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "game's neighbour words: ['is', \"loser's\", 'a', 'beating']\n",
            "market's neighbour words: ['is', \"loser's\", 'a', 'beating']\n",
            "cost's neighbour words: ['beating', 'of', 'deduction', 'the']\n",
            "loser's's neighbour words: ['game', 'market', \"loser's\", 'stock']\n",
            "stock's neighbour words: ['investing', 'a', 'is', 'market']\n",
            "of's neighbour words: ['the', 'of', 'cost', 'beating']\n",
            "beating's neighbour words: ['market', 'stock', 'investing', 'cost']\n",
            "investing's neighbour words: ['the', 'stock', 'beating', 'cost']\n",
            "a's neighbour words: ['game', 'market', \"loser's\", 'stock']\n",
            "the's neighbour words: ['is', 'of', 'beating', 'stock']\n",
            "deduction's neighbour words: ['cost', 'after', 'the', 'beating']\n",
            "is's neighbour words: ['game', 'market', \"loser's\", 'a']\n",
            "after's neighbour words: ['of', 'the', 'deduction', 'cost']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}