{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "### From 'edureka' page on web scraping - https://www.edureka.co/blog/web-scraping-with-python/\n",
    "\n",
    "## *Written By Nathanael Hitch*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful for 'pulling' large amounts of data from a website; it is a lot faster and easier than going to the website and doing it manually. Examples include:\n",
    "\n",
    "- Price Comparison: collecting data from online shopping websites to compare\n",
    "- Social Media Trending: collect data from social media websites, such as Twitter, to see what's trending\n",
    "\n",
    "Web scraping helps collect the data that's, usually, unstructured and stores it in a structured form. There are different ways to web scrape, including:\n",
    "\n",
    "- Online Services\n",
    "- APIs\n",
    "- Writing your own code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you scrape off of THIS website?\n",
    "\n",
    "While web scraping is legal, some websites allow it while others don't. To see if a website does allow web scraping, you can look at the site's \"robots.txt\" file by appending \"/robots.txt\" to the URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "\n",
    "Running web scraping code a request to the URL; the server responds by the response as a HTML or XML page. The basic steps for web scraping using Python is:\n",
    "\n",
    "1. Find the URL that you want to scrape\n",
    "2. Inspect the page\n",
    "3. Find the data you want to extract\n",
    "4. Write the code\n",
    "5. Run the code and extract the data\n",
    "6. Store the data in the required format\n",
    "\n",
    "The libraries that'll be used for web scraping are:\n",
    "\n",
    "- Selenium: web testing library, used to automate browser activities\n",
    "- BeautifulSoup: parses HTML and XML documents, creating parse trees\n",
    "- Pandas: used for data manipulation and analysis, extracting the data and store it in a desired format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Flipkart Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the URL you want to scrape:\n",
    "\n",
    "Use the Flipkart website to extract the Price, Name and Rating of the laptops.\n",
    "- https://www.flipkart.com/laptops/~buyback-guarantee-on-laptops-/pr?sid=6bo%2Cb5g&uniqBStoreParam1=val1&wid=11.productCard.PMU_V2\n",
    "\n",
    "2. Inspect the page:\n",
    "\n",
    "As the data is usually nested in tags, we need to see under which tag the data we need to scrape is. To inspect the page, right-click on the element and click \"Inspect\" and a \"Browser Inspection Box\" will open\n",
    "\n",
    "3. Find the data we want extracted\n",
    "\n",
    "Find where the Name, Price and Rating for the laptops are nested in the box. Additionally, extracting the Processor is useful as it has certain barriers which are useful to learn to work round.\n",
    "\n",
    "4. Write the code\n",
    "\n",
    "You need to install the packages required for the code below, Selenium, Pandas and BeautifulSoup4 (bs4). To install the code onto Python:\n",
    "\n",
    "- \\>>> pip install *Package Name*\n",
    "\n",
    "To install onto Anaconda:\n",
    "\n",
    "- \\>>> conda install *Package Name*\n",
    "\n",
    "Additionally, the ChromeDriver needs to be installed for selenium to use it:\n",
    "\n",
    "- Download ChromeDriver from the ChromeDriver website - https://chromedriver.chromium.org/.\n",
    "- Download the necessary version (e.g. Windows) and cut and paste the download into your WebDrivers folder in the C: drive (create a folder if needed).\n",
    "- Add the WebDrivers folder path to the PATH system variables.\n",
    "\n",
    "Selenium should be able to find and use the WebDriver.<br>\n",
    "*[Additionaly notes on 'ChromeDriver' are in NoteBook 4]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Products:\n",
      "***************************************************************\n",
      " ['Apple MacBook Air Core i5 5th Gen - (8 GB/128 GB SSD/Mac OS Sierra) MQD32HN/A A1466', 'HP 15 Core i3 6th Gen - (4 GB/1 TB HDD/Windows 10 Home) 15-be014TU Laptop', 'Lenovo Ideapad Core i5 7th Gen - (8 GB/1 TB HDD/Windows 10 Home/2 GB Graphics) IP 320-15IKB Laptop', 'Lenovo Core i5 7th Gen - (8 GB/2 TB HDD/Windows 10 Home/4 GB Graphics) IP 520 Laptop', 'Lenovo Core i5 7th Gen - (8 GB/1 TB HDD/DOS/2 GB Graphics) IP 320-15IKB Laptop'] \n",
      "***************************************************************\n",
      "\n",
      " Prices:\n",
      "***************************************************************\n",
      " ['₹66,990', '₹36,163', '₹51,990', '₹79,500', '₹56,990'] \n",
      "***************************************************************\n",
      "\n",
      " Ratings:\n",
      "***************************************************************\n",
      " ['4.7', '4.1', '4.3', '4.4', '4.3'] \n",
      "***************************************************************\n",
      "\n",
      " Processors:\n",
      "***************************************************************\n",
      " ['Intel Core i5 Processor (5th Gen)', 'Intel Core i3 Processor (6th Gen)', 'Intel Core i5 Processor (7th Gen)', 'Intel Core i5 Processor (7th Gen)', 'Intel Core i5 Processor (7th Gen)'] \n",
      "***************************************************************\n",
      "\n",
      " FINISHED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver = webdriver.Chrome(\"C:\\WebDrivers\\chromedriver\")\n",
    "# Calls the ChromeDriver so that selenium can access Chrome.\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/laptops/~buyback-guarantee-on-laptops-/pr?sid=6bo%2Cb5g&uniq\")\n",
    "# Opens a browser (Chrome in this case) with that URL.\n",
    "\n",
    "products = []\n",
    "prices = []\n",
    "ratings = []\n",
    "processors = []\n",
    "\n",
    "content = driver.page_source\n",
    "# Get's the html script from the driver's page - stuff seen in Browser Inspection Box.\n",
    "\n",
    "soup = BeautifulSoup(content)\n",
    "# Format's the content into a parse tree.\n",
    "\n",
    "findallInSoup = soup.find_all('a', href=True, attrs={'class':'_31qSD5'})\n",
    "# Finds all the <a> tags in soup that have a hyper-reference and their class equals '_31qSD5'.\n",
    "# Basically, the details about the laptops on the website.\n",
    "\n",
    "for a in findallInSoup:\n",
    "# For each <a> tag found in soup...\n",
    "    \n",
    "    name = a.find('div', attrs={'class':'_3wU53n'})\n",
    "    # Finds a <div> tag in with their class equaling '_3wU53n' - the name of the laptop.\n",
    "    \n",
    "    price = a.find('div', attrs={'class':'_1vC4OE _2rQ-NK'})\n",
    "    # Finds a <div> tag with the class equaling '_1vC4OE _2rQ-NK' - the price of the laptop.\n",
    "    \n",
    "    rating = a.find('div', attrs={'class':'hGSR34'}) # This initially didn't work; unknown why.\n",
    "    # Finds a <span> tag with the class equaling '_2_KrJI' - the rating of the laptop.\n",
    "        # OR -> rating = a.find('span', attrs={'class':'_2_KrJI'})\n",
    "            # On eudreka, the class was said to equal 'hGSR34 _2beYZw' but that didn't work.\n",
    "            \n",
    "    products.append(name.text)\n",
    "    prices.append(price.text)\n",
    "    ratings.append(rating.text)\n",
    "    # Appends each name/price/rating to its respective list.\n",
    "    \n",
    "    laptop_details = a.find_all('li', attrs={'class':'tVe95H'})\n",
    "    # Finds a <li> tag with the tag equaling 'tVe95H'.\n",
    "        # There are more than one on these tags though.        \n",
    "    for eachInfo in laptop_details:\n",
    "        if \"processor\" in eachInfo.text.lower():\n",
    "        # Find whether this part of the laptop information contains the processor.\n",
    "            processors.append(eachInfo.text)\n",
    "            # Append the processor to the processors list.\n",
    "            break\n",
    "            # 'Break' as we've found what we needed.\n",
    "    \n",
    "    # Larger portions of data can be extracted:\n",
    "    comp_dets = a.find('ul', attrs={'class':'vFw0gD'})\n",
    "    #print(\"Details:\\n\", comp_dets, \"\\n\")\n",
    "        \n",
    "# Print data:\n",
    "print(\"\\n\",\"Products:\\n***************************************************************\\n\",\\\n",
    "      products,\"\\n***************************************************************\")\n",
    "print(\"\\n\",\"Prices:\\n***************************************************************\\n\",\\\n",
    "      prices,\"\\n***************************************************************\")\n",
    "print(\"\\n\",\"Ratings:\\n***************************************************************\\n\",\\\n",
    "      ratings,\"\\n***************************************************************\")\n",
    "print(\"\\n\",\"Processors:\\n***************************************************************\\n\",\\\n",
    "      processors,\"\\n***************************************************************\")\n",
    "\n",
    "driver.quit()\n",
    "# Closes the browser that the driver opened at the start.\n",
    "\n",
    "print(\"\\n\",\"FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run the code and extract the data\n",
    "\n",
    "To run the code in command script (assuming you've set up the PATH variables for python and/or conda):\n",
    "\n",
    "- \\>>> python \"*{File Directory}* \\\\ *{File name}*.py\"\n",
    "- \\>>> conda run \"*{File Directory}* \\\\ *{File name}*.py\"\n",
    "\n",
    "The \"\" are needed if there are spaces in the file directory or name; probably best to put them in anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Store the data in a required format\n",
    "\n",
    "Once the code is run, storing the information in a file is useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Created\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Continuing the code from before: \"\"\"\n",
    "\n",
    "df = pd.DataFrame({'ProductName':products,'Price':prices,\\\n",
    "                   'Rating':ratings,'Processor':processors})\n",
    "# Creating a dataframe from the previous lists.\n",
    "\n",
    "df.to_csv('Files\\products.csv', index=False, encoding='utf-8')\n",
    "# Creating a csv file/ overwriting a previously created file, populating it with the dataframe.\n",
    "    # 'Files' folder needs to already exist.\n",
    "\n",
    "print(\"File Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless a file path is stated, the document will be saved in the same folder as the python file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
