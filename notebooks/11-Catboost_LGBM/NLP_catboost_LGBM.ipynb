{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595238614864",
   "display_name": "Python 3.7.3 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1><center> CatBoost & LGBM </center></h1>\n",
    "\n",
    "*https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/*\n",
    "\n",
    "*https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2*\n",
    "\n",
    "*https://www.kdnuggets.com/2018/03/catboost-vs-light-gbm-vs-xgboost.html*\n",
    "\n",
    "## *Written by Nathanael Hitch*\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost** and **Light GBM (LGBM)** are algorithms for gradient boosting decision trees (such as **Random Forest**). These *boosts* usually rely on data that uses **One-Hot Encoding*.\n",
    "\n",
    "# One-Hot Encoding\n",
    "\n",
    "Often, machine learning data sets will require, or at least recommend, that you prepare your data in specific ways before fitting a machine learning model. Our main example is using one-hot encoding on categorical data.\n",
    "\n",
    "Categorical data are variables that contain label values, rather than numeric values, with the number of possible values often limited to a fixed set.\n",
    "\n",
    "- A \"pet\" variable with the values: \"dog\" and \"cat\".\n",
    "- A \"color\" variable with the values: \"red\", \"green\" and \"blue\".\n",
    "- A \"place\" variable with the values: \"first\", \"second\" and \"third\".\n",
    "\n",
    "Some algorithms can work with categorical data directly; a decision tree can learn directly from categorical data with no data transform required, depending on the specific implementation.<br>\n",
    "However, many machine learning algorithms cannot operate on label data directly, requiring all input variables and output variables to be numeric.\n",
    "\n",
    "This means that categorical data must be converted to a numerical form. Additionally, if the categorical variable is an output variable, you may also want to convert predictions by the model back into a categorical form in order to present them or use them in some application.\n",
    "\n",
    "### Convert from Categorical to numerical\n",
    "\n",
    "**1. Integer Encoding**\n",
    "\n",
    "Firstly, each unique category value is assigned an integer value. For example, \"red\" is 1, \"green\" is 2, and \"blue\" is 3. This is called a **label encoding**/**integer encoding** and is easily reversible.\n",
    "\n",
    "For some variables, this may be enough; the integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship. The ordinal variables like the above \"place\" example would be a good example where a label encoding would be sufficient.\n",
    "\n",
    "**2. One-Hot Encoding**\n",
    "\n",
    "For categorical variables, where no such ordinal relationship exists, the integer encoding is not enough.<br>\n",
    "Using this encoding and allowing the model to assume a natural ordering between categories could result in poor performance or unexpected results (predictions halfway between categories).\n",
    "\n",
    "In this case, a one-hot encoding can be applied to the integer representation.\n",
    "- The integer encoded variable is removed and a new binary variable is added for each unique integer value.\n",
    "\n",
    "For the \"color\" variable example, there are 3 categories and therefore 3 binary variables are needed. A “1” value is placed in the binary variable for the color and \"0\" values for the other colors:\n",
    "\n",
    "red   = 1, 0, 0\n",
    "green = 0, 1, 0\n",
    "blue  = 0, 0, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost\n",
    "\n",
    "CatBoost has the flexibility of giving indices of categorical columns so that it can be encoded as one-hot encoding using one_hot_max_size.<br>\n",
    "(Advisable to use one-hot encoding for all features with number of different values less than or equal to the given parameter value).\n",
    "\n",
    "If you don’t pass any anything in the **cat_features argument**, CatBoost will treat all the columns as numerical variables.\n",
    "\n",
    "Note: if a column having string values is not provided in the cat_features, CatBoost throws an error. Also, a column having default int type will be treated as numeric by default, one has to specify it in cat_features to make the algorithm treat it as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d222a454d36a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}